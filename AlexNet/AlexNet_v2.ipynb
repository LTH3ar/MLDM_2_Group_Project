{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb33124d0f79549",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64431bcca2e2767",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torchmetrics import Accuracy\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13f7ab17be9bbe6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 2. Setting Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b23f8f393bd80e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Setting Device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab6e8b2d7957ce3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 3. Preparing Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8751b5498bcb3f24",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Preparing Input Data\n",
    "# prepare the dataset MNIST(1x28x28) -> (3x224x224) for AlexNet\n",
    "# Upscale the grayscale images to RGB size\n",
    "param_transform = transforms.Compose([\n",
    "    transforms.Pad(2),\n",
    "    transforms.Resize((227, 227)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to 3-channel grayscale\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.1307], std=[0.3081])  # Normalize to [-1, 1] range\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4c641463aaf86e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "train_val_dataset = datasets.MNIST(root='./dataset', train=True, transform=param_transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./dataset', train=False, transform=param_transform, download=True)\n",
    "\n",
    "# Dataset summary\n",
    "print('train_val_dataset length:', len(train_val_dataset))\n",
    "print('test_dataset length:', len(test_dataset))\n",
    "print('train_val_dataset shape:', train_val_dataset[0][0].shape)\n",
    "print('test_dataset shape:', test_dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab95b070cf5dfaa2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Split the dataset into train and validation\n",
    "train_size = int(0.8 * len(train_val_dataset))\n",
    "val_size = len(train_val_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_val_dataset, [train_size, val_size])\n",
    "\n",
    "# Dataset summary\n",
    "print('train_dataset length:', len(train_dataset))\n",
    "print('val_dataset length:', len(val_dataset))\n",
    "print('test_dataset length:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a60e29723692ef",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "if torch.cuda.is_available():\n",
    "    BATCH_SIZE = 128\n",
    "elif torch.backends.mps.is_available():\n",
    "    BATCH_SIZE = 128\n",
    "else:\n",
    "    BATCH_SIZE = 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# dataloaders summary\n",
    "print('train_loader length:', len(train_loader))\n",
    "print('val_loader length:', len(val_loader))\n",
    "print('test_loader length:', len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3489f899ec876ddd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 4. Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9353a1e991c2b2fe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the model AlexNet specific for the transformed MNIST\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # ============================================================================== #\n",
    "            # 1st conv layer\n",
    "            # input: 3x224x224 (upscaled from 1x28x28)\n",
    "            # output: 96x55x55\n",
    "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=0, ),\n",
    "            # activation function: ReLU\n",
    "            nn.ReLU(),\n",
    "            # max pooling layer with kernel size 3 and stride 2\n",
    "            # output: 96x27x27\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 2nd conv layer\n",
    "            # input: 96x27x27\n",
    "            # output: 256x27x27\n",
    "            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2),\n",
    "            # activation function: ReLU\n",
    "            nn.ReLU(),\n",
    "            # max pooling layer with kernel size 3 and stride 2\n",
    "            # output: 256x13x13\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 3rd conv layer\n",
    "            # input: 256x13x13\n",
    "            # output: 384x13x13\n",
    "            nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
    "            # activation function: ReLU\n",
    "            nn.ReLU(),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 4th conv layer\n",
    "            # input: 384x13x13\n",
    "            # output: 384x13x13\n",
    "            nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
    "            # activation function: ReLU\n",
    "            nn.ReLU(),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 5th conv layer\n",
    "            # input: 384x13x13\n",
    "            # output: 256x13x13\n",
    "            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            # activation function: ReLU\n",
    "            nn.ReLU(),\n",
    "            # max pooling layer with kernel size 3 and stride 2\n",
    "            # output: 256x6x6\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "            # ============================================================================== #\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            # flatten\n",
    "            nn.Flatten(), # 256*5*5 = 6400\n",
    "            # ============================================================================== #\n",
    "            # 1st fc layer Dense: 4096 fully connected neurons\n",
    "            nn.Dropout(p=0.5), # dropout layer with p=0.5\n",
    "            nn.Linear(in_features=256 * 6 * 6, out_features=4096), # 256*5*5\n",
    "            nn.ReLU(),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 2nd fc layer Dense: 4096 fully connected neurons\n",
    "            nn.Dropout(p=0.5), # dropout layer with p=0.5\n",
    "            nn.Linear(in_features=4096, out_features=4096), # 4096\n",
    "            nn.ReLU(),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 3rd fc layer Dense: 10 fully connected neurons\n",
    "            nn.Linear(in_features=4096, out_features=num_classes) # 4096\n",
    "            # ============================================================================== #\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12921b86faf68442",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = AlexNet().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2ab9c4fa24ae02",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Model summary\n",
    "# Detailed layer-wise summary\n",
    "#summary(model, input_size=(1, 3, 224, 224), verbose=2, device=device)\n",
    "summary(model, input_size=(1, 3, 227, 227), verbose=2, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd3027b20f1637",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Optimizer and loss function\n",
    "LEARNING_RATE = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = nn.CrossEntropyLoss().to(device)\n",
    "accuracy = Accuracy(task='multiclass', num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9d80772083c3d2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29402a8f012bba3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "# Log training process to TensorBoard\n",
    "date_time = datetime.datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "log_dir = os.path.join('train_logs', date_time)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d8d6c4006cd846",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "NUM_EPOCHS = 40\n",
    "NUM_BATCHES = len(train_loader)\n",
    "NUM_BATCHES_VAL = len(val_loader)\n",
    "NUM_BATCHES_TEST = len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dab14f410e6b066",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "if not os.path.exists('models'):\n",
    "    os.mkdir('models')\n",
    "VERSION = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4396dcaa5383e5d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "acc_train = []\n",
    "loss_train = []\n",
    "acc_val = []\n",
    "loss_val = []\n",
    "acc_test = []\n",
    "loss_test = []\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_count = epoch + 1\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Move data to device\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Forward\n",
    "        output = model(data)\n",
    "        loss = loss_function(output, target)\n",
    "        acc = accuracy(output, target)\n",
    "        \n",
    "        # convert accuracy value to percentage\n",
    "        acc = acc * 100\n",
    "        \n",
    "        # save loss and accuracy\n",
    "        acc_train.append(acc.item())\n",
    "        loss_train.append(loss.item())\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # log training\n",
    "        if batch_idx % 10 == 0: # every 100 mini-batches\n",
    "            print(f'Train Epoch: {epoch_count} [{batch_idx}/{NUM_BATCHES} ({100. * batch_idx / NUM_BATCHES:.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "            #writer.add_scalar('Train Loss (Every 10 batch)', loss.item(), epoch * NUM_BATCHES + batch_idx)\n",
    "            #writer.add_scalar('Train Accuracy (Every 10 batch)', acc.item(), epoch * NUM_BATCHES + batch_idx)\n",
    "            print(f'Train Epoch: {epoch_count} [{batch_idx}/{NUM_BATCHES} ({100. * batch_idx / NUM_BATCHES:.0f}%)]\\tAccuracy: {acc.item():.6f}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            # Move data to device\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # Forward\n",
    "            output = model(data)\n",
    "            loss = loss_function(output, target)\n",
    "            acc = accuracy(output, target)\n",
    "            \n",
    "            # convert accuracy value to percentage\n",
    "            acc = acc * 100\n",
    "            \n",
    "            # save loss and accuracy\n",
    "            acc_val.append(acc.item())\n",
    "            loss_val.append(loss.item())\n",
    "\n",
    "            # log validation\n",
    "            if batch_idx % 10 == 0: # every 100 mini-batches\n",
    "                print(f'Validation Epoch: {epoch_count} [{batch_idx}/{NUM_BATCHES_VAL} ({100. * batch_idx / NUM_BATCHES_VAL:.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "                #writer.add_scalar('Validation Loss (Every 10 batch)', loss.item(), epoch * NUM_BATCHES_VAL + batch_idx)\n",
    "                #writer.add_scalar('Validation Accuracy (Every 10 batch)', acc.item(), epoch * NUM_BATCHES_VAL + batch_idx)\n",
    "                print(f'Validation Epoch: {epoch_count} [{batch_idx}/{NUM_BATCHES_VAL} ({100. * batch_idx / NUM_BATCHES_VAL:.0f}%)]\\tAccuracy: {acc.item():.6f}')\n",
    "                \n",
    "    # Calculate average loss and accuracy over an epoch\n",
    "    avg_loss_train = np.mean(loss_train)\n",
    "    avg_acc_train = np.mean(acc_train)\n",
    "    avg_loss_val = np.mean(loss_val)\n",
    "    avg_acc_val = np.mean(acc_val)\n",
    "    \n",
    "    # Log average loss and accuracy of an epoch\n",
    "    writer.add_scalars(main_tag='Accuracy train/validation', tag_scalar_dict={'TRAIN': avg_acc_train, 'VAL': avg_acc_val}, global_step=epoch_count) \n",
    "    writer.add_scalars(main_tag='Loss train/validation', tag_scalar_dict={'TRAIN': avg_loss_train, 'VAL': avg_loss_val}, global_step=epoch_count)\n",
    "    print(f'Epoch: {epoch_count}\\tAverage Train Loss: {avg_loss_train:.6f}\\tAverage Train Accuracy: {avg_acc_train:.6f}')\n",
    "    print(f'Epoch: {epoch_count}\\tAverage Validation Loss: {avg_loss_val:.6f}\\tAverage Validation Accuracy: {avg_acc_val:.6f}')\n",
    "\n",
    "    # Test\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            # Move data to device\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "    \n",
    "            # Forward\n",
    "            output = model(data)\n",
    "            loss = loss_function(output, target)\n",
    "            acc = accuracy(output, target)\n",
    "            \n",
    "            # convert accuracy value to percentage\n",
    "            acc = acc * 100\n",
    "    \n",
    "            # Log loss and accuracy\n",
    "            acc_test.append(acc.item())\n",
    "            loss_test.append(loss.item())\n",
    "            \n",
    "    # log test every epoch\n",
    "    avg_loss_test = np.mean(loss_test)\n",
    "    avg_acc_test = np.mean(acc_test)\n",
    "    #writer.add_scalars(main_tag='Accuracy & Loss/test', tag_scalar_dict={'Accuracy': avg_acc_test, 'Loss': avg_loss_test}, global_step=epoch_count)\n",
    "    writer.add_scalar('Accuracy/test', avg_acc_test, epoch_count)\n",
    "    writer.add_scalar('Loss/test', avg_loss_test, epoch_count)\n",
    "    print(f'Epoch: {epoch_count}\\tAverage Test Loss: {avg_loss_test:.6f}\\tAverage Test Accuracy: {avg_acc_test:.6f}')\n",
    "    if (epoch_count) % 10 == 0:\n",
    "        MODEL_NAME = f'LeNet5_v{VERSION}_{date_time}.pth'\n",
    "        torch.save(model.state_dict(), os.path.join('models', MODEL_NAME))\n",
    "        print(f'Saved PyTorch Model State to {MODEL_NAME}')\n",
    "        VERSION += 1\n",
    "        \n",
    "            \n",
    "    \n",
    "# clear cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "elif torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "else:\n",
    "    torch.empty_cache()\n",
    "features = None\n",
    "targets = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46a8a22e81d1077",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Close the writer\n",
    "features = None\n",
    "targets = None\n",
    "\n",
    "# Close the writer\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5ae0ffa8e8c4a2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "model_loaded = None\n",
    "\n",
    "# release all loaders\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "# release all variables\n",
    "optimizer = None\n",
    "loss_fn = None\n",
    "accuracy = None\n",
    "\n",
    "print('Released all variables')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
