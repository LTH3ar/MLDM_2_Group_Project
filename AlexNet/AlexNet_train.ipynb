{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Importing Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb33124d0f79549"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torchmetrics import Accuracy\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e64431bcca2e2767"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Setting Device"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c13f7ab17be9bbe6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setting Device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65b23f8f393bd80e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Preparing Input Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ab6e8b2d7957ce3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Preparing Input Data\n",
    "# prepare the dataset MNIST(1x28x28) -> (3x224x224) for AlexNet\n",
    "# Upscale the grayscale images to RGB size\n",
    "param_transform = transforms.Compose([\n",
    "    transforms.Pad(2),\n",
    "    transforms.Resize((227, 227)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to 3-channel grayscale\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.1307], std=[0.3081])  # Normalize to [-1, 1] range\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8751b5498bcb3f24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "train_val_dataset = datasets.MNIST(root='./dataset', train=True, transform=param_transform, download=True)\n",
    "\n",
    "# Dataset summary\n",
    "print('train_val_dataset length:', len(train_val_dataset))\n",
    "print('train_val_dataset shape:', train_val_dataset[0][0].shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c4c641463aaf86e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the dataset into train and validation\n",
    "train_size = int(0.8 * len(train_val_dataset))\n",
    "val_size = len(train_val_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_val_dataset, [train_size, val_size])\n",
    "\n",
    "# Dataset summary\n",
    "print('train_dataset length:', len(train_dataset))\n",
    "print('val_dataset length:', len(val_dataset))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab95b070cf5dfaa2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "if torch.cuda.is_available():\n",
    "    BATCH_SIZE = 128\n",
    "elif torch.backends.mps.is_available():\n",
    "    BATCH_SIZE = 128\n",
    "else:\n",
    "    BATCH_SIZE = 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# dataloaders summary\n",
    "print('train_loader length:', len(train_loader))\n",
    "print('val_loader length:', len(val_loader))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7a60e29723692ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Defining Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3489f899ec876ddd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the model AlexNet specific for the transformed MNIST\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # ============================================================================== #\n",
    "            # 1st conv layer\n",
    "            # input: 3x224x224 (upscaled from 1x28x28)\n",
    "            # output: 96x55x55\n",
    "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=0, ),\n",
    "            # activation function: ReLU\n",
    "            nn.ReLU(),\n",
    "            # max pooling layer with kernel size 3 and stride 2\n",
    "            # output: 96x27x27\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 2nd conv layer\n",
    "            # input: 96x27x27\n",
    "            # output: 256x27x27\n",
    "            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2),\n",
    "            # activation function: ReLU\n",
    "            nn.ReLU(),\n",
    "            # max pooling layer with kernel size 3 and stride 2\n",
    "            # output: 256x13x13\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 3rd conv layer\n",
    "            # input: 256x13x13\n",
    "            # output: 384x13x13\n",
    "            nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
    "            # activation function: ReLU\n",
    "            nn.ReLU(),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 4th conv layer\n",
    "            # input: 384x13x13\n",
    "            # output: 384x13x13\n",
    "            nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
    "            # activation function: ReLU\n",
    "            nn.ReLU(),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 5th conv layer\n",
    "            # input: 384x13x13\n",
    "            # output: 256x13x13\n",
    "            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            # activation function: ReLU\n",
    "            nn.ReLU(),\n",
    "            # max pooling layer with kernel size 3 and stride 2\n",
    "            # output: 256x6x6\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "            # ============================================================================== #\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            # flatten\n",
    "            nn.Flatten(), # 256*5*5 = 6400\n",
    "            # ============================================================================== #\n",
    "            # 1st fc layer Dense: 4096 fully connected neurons\n",
    "            nn.Dropout(p=0.5), # dropout layer with p=0.5\n",
    "            nn.Linear(in_features=256 * 6 * 6, out_features=4096), # 256*5*5\n",
    "            nn.ReLU(),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 2nd fc layer Dense: 4096 fully connected neurons\n",
    "            nn.Dropout(p=0.5), # dropout layer with p=0.5\n",
    "            nn.Linear(in_features=4096, out_features=4096), # 4096\n",
    "            nn.ReLU(),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 3rd fc layer Dense: 10 fully connected neurons\n",
    "            nn.Linear(in_features=4096, out_features=num_classes) # 4096\n",
    "            # ============================================================================== #\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9353a1e991c2b2fe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = AlexNet().to(device)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12921b86faf68442"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Model summary\n",
    "# Detailed layer-wise summary\n",
    "#summary(model, input_size=(1, 3, 224, 224), verbose=2, device=device)\n",
    "summary(model, input_size=(1, 3, 227, 227), verbose=2, device=device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae2ab9c4fa24ae02"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Optimizer and loss function\n",
    "LEARNING_RATE = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = nn.CrossEntropyLoss().to(device)\n",
    "accuracy = Accuracy(task='multiclass', num_classes=10).to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebd3027b20f1637"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c9d80772083c3d2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training\n",
    "# Log training process to TensorBoard\n",
    "date_time = datetime.datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "log_dir = os.path.join('train_logs', date_time)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d29402a8f012bba3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "NUM_EPOCHS = 40\n",
    "NUM_BATCHES = len(train_loader)\n",
    "NUM_BATCHES_VAL = len(val_loader)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1d8d6c4006cd846"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "if not os.path.exists('models'):\n",
    "    os.mkdir('models')\n",
    "VERSION = 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3dab14f410e6b066"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    y_pred_train = []\n",
    "    y_true_train = []\n",
    "    y_pred_val = []\n",
    "    y_true_val = []\n",
    "    y_pred_test = []\n",
    "    y_true_test = []\n",
    "    epoch_count = epoch + 1\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Move data to device\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Forward\n",
    "        output = model(data)\n",
    "        loss = loss_function(output, target)\n",
    "        \n",
    "        # add y_pred and y_true\n",
    "        y_pred_train.append(output)\n",
    "        y_true_train.append(target)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # log training\n",
    "        print(f'Train Epoch: {epoch_count} [{batch_idx}/{NUM_BATCHES} ({100. * batch_idx / NUM_BATCHES:.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "    \n",
    "    # Calculate average loss and accuracy over an epoch\n",
    "    avg_loss_train = loss_function(torch.cat(y_pred_train), torch.cat(y_true_train)).item()\n",
    "    avg_acc_train = accuracy(torch.cat(y_pred_train), torch.cat(y_true_train))*100\n",
    "    \n",
    "    print(f'Epoch: {epoch_count}\\tAverage Train Loss: {avg_loss_train:.6f}\\tAverage Train Accuracy: {avg_acc_train:.6f}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            # Move data to device\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # Forward\n",
    "            output = model(data)\n",
    "            \n",
    "            # add y_pred and y_true\n",
    "            y_pred_val.append(output)\n",
    "            y_true_val.append(target)\n",
    "\n",
    "            # log validation\n",
    "            print(f'Validation Epoch: {epoch_count} [{batch_idx}/{NUM_BATCHES_VAL} ({100. * batch_idx / NUM_BATCHES_VAL:.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "                \n",
    "    # Calculate average loss and accuracy over an epoch\n",
    "    avg_loss_val = loss_function(torch.cat(y_pred_val), torch.cat(y_true_val)).item()\n",
    "    avg_acc_val = accuracy(torch.cat(y_pred_val), torch.cat(y_true_val))*100\n",
    "    \n",
    "    print(f'Epoch: {epoch_count}\\tAverage Validation Loss: {avg_loss_val:.6f}\\tAverage Validation Accuracy: {avg_acc_val:.6f}')\n",
    "\n",
    "    # Log average loss and accuracy of an epoch \n",
    "    writer.add_scalars(main_tag='Accuracy train/validation', tag_scalar_dict={'TRAIN': avg_acc_train, 'VAL': avg_acc_val}, global_step=epoch_count)\n",
    "    writer.add_scalars(main_tag='Loss train/validation', tag_scalar_dict={'TRAIN': avg_loss_train, 'VAL': avg_loss_val}, global_step=epoch_count)\n",
    "    \n",
    "\n",
    "    if epoch_count % 10 == 0:\n",
    "        MODEL_NAME = f'LeNet5_v{VERSION}_{date_time}.pth'\n",
    "        torch.save(model.state_dict(), os.path.join('models', MODEL_NAME))\n",
    "        print(f'Saved PyTorch Model State to {MODEL_NAME}')\n",
    "        VERSION += 1\n",
    "        \n",
    "# clear cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "elif torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "else:\n",
    "    torch.empty_cache()\n",
    "features = None\n",
    "targets = None"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4396dcaa5383e5d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Close the writer\n",
    "writer.flush()\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c46a8a22e81d1077"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = None\n",
    "model_loaded = None\n",
    "\n",
    "# release all loaders\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "# release all variables\n",
    "optimizer = None\n",
    "loss_fn = None\n",
    "accuracy = None\n",
    "\n",
    "print('Released all variables')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f5ae0ffa8e8c4a2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
