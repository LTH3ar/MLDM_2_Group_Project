{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Importing Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a5c64051bd059b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-01 00:41:29.126392: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-01 00:41:29.126429: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-01 00:41:29.127465: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-01 00:41:29.132519: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-01 00:41:29.845111: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torchmetrics import Accuracy\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-31T17:41:31.103502475Z",
     "start_time": "2023-12-31T17:41:27.980462887Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Setting Device"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b57f7f91e5c6b06a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Setting Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T17:41:31.178559533Z",
     "start_time": "2023-12-31T17:41:31.175343034Z"
    }
   },
   "id": "47189a400c9fa3c7",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Preparing Input Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c05cef9cb03b952"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Preparing Input Data\n",
    "# prepare the dataset MNIST(1x28x28) -> (3x224x224) for AlexNet\n",
    "# Upscale the grayscale images to RGB size\n",
    "upscale_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to 3-channel grayscale\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.1307], std=[0.3081])  # Normalize to [-1, 1] range\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T17:41:31.205149560Z",
     "start_time": "2023-12-31T17:41:31.178605029Z"
    }
   },
   "id": "43eb54b34ab07e86",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_val_dataset length: 60000\n",
      "test_dataset length: 10000\n",
      "train_val_dataset shape: torch.Size([3, 224, 224])\n",
      "test_dataset shape: torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "train_val_dataset = datasets.MNIST(root='./dataset', train=True, transform=upscale_transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./dataset', train=False, transform=upscale_transform, download=True)\n",
    "\n",
    "# Dataset summary\n",
    "print('train_val_dataset length:', len(train_val_dataset))\n",
    "print('test_dataset length:', len(test_dataset))\n",
    "print('train_val_dataset shape:', train_val_dataset[0][0].shape)\n",
    "print('test_dataset shape:', test_dataset[0][0].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T17:41:31.246209739Z",
     "start_time": "2023-12-31T17:41:31.197433092Z"
    }
   },
   "id": "54b4b930857f8cff",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset length: 48000\n",
      "val_dataset length: 12000\n",
      "test_dataset length: 10000\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train and validation\n",
    "train_size = int(0.8 * len(train_val_dataset))\n",
    "val_size = len(train_val_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_val_dataset, [train_size, val_size])\n",
    "\n",
    "# Dataset summary\n",
    "print('train_dataset length:', len(train_dataset))\n",
    "print('val_dataset length:', len(val_dataset))\n",
    "print('test_dataset length:', len(test_dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T17:41:31.290616073Z",
     "start_time": "2023-12-31T17:41:31.248526445Z"
    }
   },
   "id": "e37052e65ce41d98",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader length: 375\n",
      "val_loader length: 94\n",
      "test_loader length: 79\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "BATCH_SIZE = 128 if torch.cuda.is_available() else 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# dataloaders summary\n",
    "print('train_loader length:', len(train_loader))\n",
    "print('val_loader length:', len(val_loader))\n",
    "print('test_loader length:', len(test_loader))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T17:41:31.291391240Z",
     "start_time": "2023-12-31T17:41:31.290444388Z"
    }
   },
   "id": "2408fdd9969a3431",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Defining Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b4a9d85fc9feec9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define the model AlexNet specific for the transformed MNIST\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # ============================================================================== #\n",
    "            # 1st conv layer\n",
    "            # input: 3x224x224 (upscaled from 1x28x28)\n",
    "            # output: 96x55x55\n",
    "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=0, ),\n",
    "            # activation function: ReLU\n",
    "            nn.ReLU(),\n",
    "            # max pooling layer with kernel size 3 and stride 2\n",
    "            # output: 96x27x27\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 2nd conv layer\n",
    "            # input: 96x27x27\n",
    "            # output: 256x27x27\n",
    "            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2),\n",
    "            # activation function: ReLU\n",
    "            nn.ReLU(),\n",
    "            # max pooling layer with kernel size 3 and stride 2\n",
    "            # output: 256x13x13\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 3rd conv layer\n",
    "            # input: 256x13x13\n",
    "            # output: 384x13x13\n",
    "            nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
    "            # activation function: ReLU\n",
    "            nn.ReLU(),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 4th conv layer\n",
    "            # input: 384x13x13\n",
    "            # output: 384x13x13\n",
    "            nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
    "            # activation function: ReLU\n",
    "            nn.ReLU(),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 5th conv layer\n",
    "            # input: 384x13x13\n",
    "            # output: 256x13x13\n",
    "            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            # activation function: ReLU\n",
    "            nn.ReLU(),\n",
    "            # max pooling layer with kernel size 3 and stride 2\n",
    "            # output: 256x6x6\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "            # ============================================================================== #\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            # flatten\n",
    "            nn.Flatten(), # 256*5*5 = 6400\n",
    "            # ============================================================================== #\n",
    "            # 1st fc layer Dense: 4096 fully connected neurons\n",
    "            nn.Dropout(p=0.5), # dropout layer with p=0.5\n",
    "            nn.Linear(in_features=256 * 5 * 5, out_features=4096), # 256*5*5\n",
    "            nn.ReLU(),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 2nd fc layer Dense: 4096 fully connected neurons\n",
    "            nn.Dropout(p=0.5), # dropout layer with p=0.5\n",
    "            nn.Linear(in_features=4096, out_features=4096), # 4096\n",
    "            nn.ReLU(),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 3rd fc layer Dense: 10 fully connected neurons\n",
    "            nn.Linear(in_features=4096, out_features=num_classes) # 4096\n",
    "            # ============================================================================== #\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T17:41:31.291717849Z",
     "start_time": "2023-12-31T17:41:31.290570457Z"
    }
   },
   "id": "459d3e93ddc78a53",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU()\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Dropout(p=0.5, inplace=False)\n",
      "    (2): Linear(in_features=6400, out_features=4096, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "    (5): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = AlexNet().to(device)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T17:41:31.623646792Z",
     "start_time": "2023-12-31T17:41:31.290641351Z"
    }
   },
   "id": "98615f7aa89f388c",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "AlexNet                                  [1, 10]                   --\n",
      "├─Sequential: 1-1                        [1, 256, 5, 5]            --\n",
      "│    └─0.weight                                                    ├─34,848\n",
      "│    └─0.bias                                                      ├─96\n",
      "│    └─3.weight                                                    ├─614,400\n",
      "│    └─3.bias                                                      ├─256\n",
      "│    └─6.weight                                                    ├─884,736\n",
      "│    └─6.bias                                                      ├─384\n",
      "│    └─8.weight                                                    ├─1,327,104\n",
      "│    └─8.bias                                                      ├─384\n",
      "│    └─10.weight                                                   ├─884,736\n",
      "│    └─10.bias                                                     └─256\n",
      "│    └─Conv2d: 2-1                       [1, 96, 54, 54]           34,944\n",
      "│    │    └─weight                                                 ├─34,848\n",
      "│    │    └─bias                                                   └─96\n",
      "│    └─ReLU: 2-2                         [1, 96, 54, 54]           --\n",
      "│    └─MaxPool2d: 2-3                    [1, 96, 26, 26]           --\n",
      "│    └─Conv2d: 2-4                       [1, 256, 26, 26]          614,656\n",
      "│    │    └─weight                                                 ├─614,400\n",
      "│    │    └─bias                                                   └─256\n",
      "│    └─ReLU: 2-5                         [1, 256, 26, 26]          --\n",
      "│    └─MaxPool2d: 2-6                    [1, 256, 12, 12]          --\n",
      "│    └─Conv2d: 2-7                       [1, 384, 12, 12]          885,120\n",
      "│    │    └─weight                                                 ├─884,736\n",
      "│    │    └─bias                                                   └─384\n",
      "│    └─ReLU: 2-8                         [1, 384, 12, 12]          --\n",
      "│    └─Conv2d: 2-9                       [1, 384, 12, 12]          1,327,488\n",
      "│    │    └─weight                                                 ├─1,327,104\n",
      "│    │    └─bias                                                   └─384\n",
      "│    └─ReLU: 2-10                        [1, 384, 12, 12]          --\n",
      "│    └─Conv2d: 2-11                      [1, 256, 12, 12]          884,992\n",
      "│    │    └─weight                                                 ├─884,736\n",
      "│    │    └─bias                                                   └─256\n",
      "│    └─ReLU: 2-12                        [1, 256, 12, 12]          --\n",
      "│    └─MaxPool2d: 2-13                   [1, 256, 5, 5]            --\n",
      "├─Sequential: 1-2                        [1, 10]                   --\n",
      "│    └─2.weight                                                    ├─26,214,400\n",
      "│    └─2.bias                                                      ├─4,096\n",
      "│    └─5.weight                                                    ├─16,777,216\n",
      "│    └─5.bias                                                      ├─4,096\n",
      "│    └─7.weight                                                    ├─40,960\n",
      "│    └─7.bias                                                      └─10\n",
      "│    └─Flatten: 2-14                     [1, 6400]                 --\n",
      "│    └─Dropout: 2-15                     [1, 6400]                 --\n",
      "│    └─Linear: 2-16                      [1, 4096]                 26,218,496\n",
      "│    │    └─weight                                                 ├─26,214,400\n",
      "│    │    └─bias                                                   └─4,096\n",
      "│    └─ReLU: 2-17                        [1, 4096]                 --\n",
      "│    └─Dropout: 2-18                     [1, 4096]                 --\n",
      "│    └─Linear: 2-19                      [1, 4096]                 16,781,312\n",
      "│    │    └─weight                                                 ├─16,777,216\n",
      "│    │    └─bias                                                   └─4,096\n",
      "│    └─ReLU: 2-20                        [1, 4096]                 --\n",
      "│    └─Linear: 2-21                      [1, 10]                   40,970\n",
      "│    │    └─weight                                                 ├─40,960\n",
      "│    │    └─bias                                                   └─10\n",
      "==========================================================================================\n",
      "Total params: 46,787,978\n",
      "Trainable params: 46,787,978\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 1.01\n",
      "==========================================================================================\n",
      "Input size (MB): 0.60\n",
      "Forward/backward pass size (MB): 4.87\n",
      "Params size (MB): 187.15\n",
      "Estimated Total Size (MB): 192.62\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAlexNet                                  [1, 10]                   --\n├─Sequential: 1-1                        [1, 256, 5, 5]            --\n│    └─0.weight                                                    ├─34,848\n│    └─0.bias                                                      ├─96\n│    └─3.weight                                                    ├─614,400\n│    └─3.bias                                                      ├─256\n│    └─6.weight                                                    ├─884,736\n│    └─6.bias                                                      ├─384\n│    └─8.weight                                                    ├─1,327,104\n│    └─8.bias                                                      ├─384\n│    └─10.weight                                                   ├─884,736\n│    └─10.bias                                                     └─256\n│    └─Conv2d: 2-1                       [1, 96, 54, 54]           34,944\n│    │    └─weight                                                 ├─34,848\n│    │    └─bias                                                   └─96\n│    └─ReLU: 2-2                         [1, 96, 54, 54]           --\n│    └─MaxPool2d: 2-3                    [1, 96, 26, 26]           --\n│    └─Conv2d: 2-4                       [1, 256, 26, 26]          614,656\n│    │    └─weight                                                 ├─614,400\n│    │    └─bias                                                   └─256\n│    └─ReLU: 2-5                         [1, 256, 26, 26]          --\n│    └─MaxPool2d: 2-6                    [1, 256, 12, 12]          --\n│    └─Conv2d: 2-7                       [1, 384, 12, 12]          885,120\n│    │    └─weight                                                 ├─884,736\n│    │    └─bias                                                   └─384\n│    └─ReLU: 2-8                         [1, 384, 12, 12]          --\n│    └─Conv2d: 2-9                       [1, 384, 12, 12]          1,327,488\n│    │    └─weight                                                 ├─1,327,104\n│    │    └─bias                                                   └─384\n│    └─ReLU: 2-10                        [1, 384, 12, 12]          --\n│    └─Conv2d: 2-11                      [1, 256, 12, 12]          884,992\n│    │    └─weight                                                 ├─884,736\n│    │    └─bias                                                   └─256\n│    └─ReLU: 2-12                        [1, 256, 12, 12]          --\n│    └─MaxPool2d: 2-13                   [1, 256, 5, 5]            --\n├─Sequential: 1-2                        [1, 10]                   --\n│    └─2.weight                                                    ├─26,214,400\n│    └─2.bias                                                      ├─4,096\n│    └─5.weight                                                    ├─16,777,216\n│    └─5.bias                                                      ├─4,096\n│    └─7.weight                                                    ├─40,960\n│    └─7.bias                                                      └─10\n│    └─Flatten: 2-14                     [1, 6400]                 --\n│    └─Dropout: 2-15                     [1, 6400]                 --\n│    └─Linear: 2-16                      [1, 4096]                 26,218,496\n│    │    └─weight                                                 ├─26,214,400\n│    │    └─bias                                                   └─4,096\n│    └─ReLU: 2-17                        [1, 4096]                 --\n│    └─Dropout: 2-18                     [1, 4096]                 --\n│    └─Linear: 2-19                      [1, 4096]                 16,781,312\n│    │    └─weight                                                 ├─16,777,216\n│    │    └─bias                                                   └─4,096\n│    └─ReLU: 2-20                        [1, 4096]                 --\n│    └─Linear: 2-21                      [1, 10]                   40,970\n│    │    └─weight                                                 ├─40,960\n│    │    └─bias                                                   └─10\n==========================================================================================\nTotal params: 46,787,978\nTrainable params: 46,787,978\nNon-trainable params: 0\nTotal mult-adds (Units.GIGABYTES): 1.01\n==========================================================================================\nInput size (MB): 0.60\nForward/backward pass size (MB): 4.87\nParams size (MB): 187.15\nEstimated Total Size (MB): 192.62\n=========================================================================================="
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model summary\n",
    "# Detailed layer-wise summary\n",
    "summary(model, input_size=(1, 3, 224, 224), verbose=2, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T17:41:31.731797158Z",
     "start_time": "2023-12-31T17:41:31.623056204Z"
    }
   },
   "id": "c628ab4d26ae84d5",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Optimizer and loss function\n",
    "LEARNING_RATE = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "accuracy = Accuracy(task='multiclass', num_classes=10).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T17:41:31.736202467Z",
     "start_time": "2023-12-31T17:41:31.732778285Z"
    }
   },
   "id": "f9d48a09cc8fecc4",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e68e261f5fc5bec"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Training\n",
    "# Log training process to TensorBoard\n",
    "date_time = datetime.datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "log_dir = os.path.join('train_logs', date_time)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T17:41:31.777135266Z",
     "start_time": "2023-12-31T17:41:31.736226583Z"
    }
   },
   "id": "f0e84690ff381c92",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "NUM_EPOCHS = 12\n",
    "BATCH_SIZE = 128 if torch.cuda.is_available() else 64\n",
    "NUM_BATCHES = len(train_loader)\n",
    "NUM_BATCHES_VAL = len(val_loader)\n",
    "NUM_BATCHES_TEST = len(test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T17:41:31.777660651Z",
     "start_time": "2023-12-31T17:41:31.777068971Z"
    }
   },
   "id": "160833a284c81c6f",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/12], Step [0/375], Loss: 2.3033\n",
      "Epoch [1/12], Step [0/375], Accuracy: 0.0625\n",
      "Epoch [1/12], Step [10/375], Loss: 2.3012\n",
      "Epoch [1/12], Step [10/375], Accuracy: 0.1094\n",
      "Epoch [1/12], Step [20/375], Loss: 2.2532\n",
      "Epoch [1/12], Step [20/375], Accuracy: 0.2656\n",
      "Epoch [1/12], Step [30/375], Loss: 1.5509\n",
      "Epoch [1/12], Step [30/375], Accuracy: 0.4688\n",
      "Epoch [1/12], Step [40/375], Loss: 1.1978\n",
      "Epoch [1/12], Step [40/375], Accuracy: 0.6016\n",
      "Epoch [1/12], Step [50/375], Loss: 0.9065\n",
      "Epoch [1/12], Step [50/375], Accuracy: 0.6797\n",
      "Epoch [1/12], Step [60/375], Loss: 0.5111\n",
      "Epoch [1/12], Step [60/375], Accuracy: 0.8672\n",
      "Epoch [1/12], Step [70/375], Loss: 0.4569\n",
      "Epoch [1/12], Step [70/375], Accuracy: 0.8984\n",
      "Epoch [1/12], Step [80/375], Loss: 0.3792\n",
      "Epoch [1/12], Step [80/375], Accuracy: 0.9141\n",
      "Epoch [1/12], Step [90/375], Loss: 0.2063\n",
      "Epoch [1/12], Step [90/375], Accuracy: 0.9375\n",
      "Epoch [1/12], Step [100/375], Loss: 0.3105\n",
      "Epoch [1/12], Step [100/375], Accuracy: 0.9219\n",
      "Epoch [1/12], Step [110/375], Loss: 0.1635\n",
      "Epoch [1/12], Step [110/375], Accuracy: 0.9453\n",
      "Epoch [1/12], Step [120/375], Loss: 0.2659\n",
      "Epoch [1/12], Step [120/375], Accuracy: 0.8906\n",
      "Epoch [1/12], Step [130/375], Loss: 0.1359\n",
      "Epoch [1/12], Step [130/375], Accuracy: 0.9609\n",
      "Epoch [1/12], Step [140/375], Loss: 0.3065\n",
      "Epoch [1/12], Step [140/375], Accuracy: 0.9375\n",
      "Epoch [1/12], Step [150/375], Loss: 0.1120\n",
      "Epoch [1/12], Step [150/375], Accuracy: 0.9609\n",
      "Epoch [1/12], Step [160/375], Loss: 0.1703\n",
      "Epoch [1/12], Step [160/375], Accuracy: 0.9609\n",
      "Epoch [1/12], Step [170/375], Loss: 0.1881\n",
      "Epoch [1/12], Step [170/375], Accuracy: 0.9141\n",
      "Epoch [1/12], Step [180/375], Loss: 0.1955\n",
      "Epoch [1/12], Step [180/375], Accuracy: 0.9219\n",
      "Epoch [1/12], Step [190/375], Loss: 0.1064\n",
      "Epoch [1/12], Step [190/375], Accuracy: 0.9688\n",
      "Epoch [1/12], Step [200/375], Loss: 0.1213\n",
      "Epoch [1/12], Step [200/375], Accuracy: 0.9766\n",
      "Epoch [1/12], Step [210/375], Loss: 0.1777\n",
      "Epoch [1/12], Step [210/375], Accuracy: 0.9453\n",
      "Epoch [1/12], Step [220/375], Loss: 0.2044\n",
      "Epoch [1/12], Step [220/375], Accuracy: 0.9375\n",
      "Epoch [1/12], Step [230/375], Loss: 0.1226\n",
      "Epoch [1/12], Step [230/375], Accuracy: 0.9531\n",
      "Epoch [1/12], Step [240/375], Loss: 0.2594\n",
      "Epoch [1/12], Step [240/375], Accuracy: 0.9297\n",
      "Epoch [1/12], Step [250/375], Loss: 0.2072\n",
      "Epoch [1/12], Step [250/375], Accuracy: 0.9453\n",
      "Epoch [1/12], Step [260/375], Loss: 0.0933\n",
      "Epoch [1/12], Step [260/375], Accuracy: 0.9688\n",
      "Epoch [1/12], Step [270/375], Loss: 0.0868\n",
      "Epoch [1/12], Step [270/375], Accuracy: 0.9609\n",
      "Epoch [1/12], Step [280/375], Loss: 0.1073\n",
      "Epoch [1/12], Step [280/375], Accuracy: 0.9531\n",
      "Epoch [1/12], Step [290/375], Loss: 0.4087\n",
      "Epoch [1/12], Step [290/375], Accuracy: 0.9375\n",
      "Epoch [1/12], Step [300/375], Loss: 0.1345\n",
      "Epoch [1/12], Step [300/375], Accuracy: 0.9609\n",
      "Epoch [1/12], Step [310/375], Loss: 0.1167\n",
      "Epoch [1/12], Step [310/375], Accuracy: 0.9531\n",
      "Epoch [1/12], Step [320/375], Loss: 0.0588\n",
      "Epoch [1/12], Step [320/375], Accuracy: 0.9766\n",
      "Epoch [1/12], Step [330/375], Loss: 0.1243\n",
      "Epoch [1/12], Step [330/375], Accuracy: 0.9844\n",
      "Epoch [1/12], Step [340/375], Loss: 0.0794\n",
      "Epoch [1/12], Step [340/375], Accuracy: 0.9844\n",
      "Epoch [1/12], Step [350/375], Loss: 0.1120\n",
      "Epoch [1/12], Step [350/375], Accuracy: 0.9688\n",
      "Epoch [1/12], Step [360/375], Loss: 0.1052\n",
      "Epoch [1/12], Step [360/375], Accuracy: 0.9766\n",
      "Epoch [1/12], Step [370/375], Loss: 0.1250\n",
      "Epoch [1/12], Step [370/375], Accuracy: 0.9531\n",
      "Epoch [1/12], Step [0/94], Loss: 0.0276 Accuracy: 0.9922\n",
      "Epoch [1/12], Step [10/94], Loss: 0.0766 Accuracy: 0.9766\n",
      "Epoch [1/12], Step [20/94], Loss: 0.0554 Accuracy: 0.9844\n",
      "Epoch [1/12], Step [30/94], Loss: 0.0836 Accuracy: 0.9844\n",
      "Epoch [1/12], Step [40/94], Loss: 0.0953 Accuracy: 0.9688\n",
      "Epoch [1/12], Step [50/94], Loss: 0.0933 Accuracy: 0.9766\n",
      "Epoch [1/12], Step [60/94], Loss: 0.1540 Accuracy: 0.9609\n",
      "Epoch [1/12], Step [70/94], Loss: 0.1125 Accuracy: 0.9609\n",
      "Epoch [1/12], Step [80/94], Loss: 0.0425 Accuracy: 0.9844\n",
      "Epoch [1/12], Step [90/94], Loss: 0.0744 Accuracy: 0.9766\n",
      "Epoch [2/12], Step [0/375], Loss: 0.1206\n",
      "Epoch [2/12], Step [0/375], Accuracy: 0.9531\n",
      "Epoch [2/12], Step [10/375], Loss: 0.0496\n",
      "Epoch [2/12], Step [10/375], Accuracy: 0.9766\n",
      "Epoch [2/12], Step [20/375], Loss: 0.0900\n",
      "Epoch [2/12], Step [20/375], Accuracy: 0.9609\n",
      "Epoch [2/12], Step [30/375], Loss: 0.0944\n",
      "Epoch [2/12], Step [30/375], Accuracy: 0.9609\n",
      "Epoch [2/12], Step [40/375], Loss: 0.0724\n",
      "Epoch [2/12], Step [40/375], Accuracy: 0.9609\n",
      "Epoch [2/12], Step [50/375], Loss: 0.0553\n",
      "Epoch [2/12], Step [50/375], Accuracy: 0.9766\n",
      "Epoch [2/12], Step [60/375], Loss: 0.1808\n",
      "Epoch [2/12], Step [60/375], Accuracy: 0.9531\n",
      "Epoch [2/12], Step [70/375], Loss: 0.0621\n",
      "Epoch [2/12], Step [70/375], Accuracy: 0.9766\n",
      "Epoch [2/12], Step [80/375], Loss: 0.2317\n",
      "Epoch [2/12], Step [80/375], Accuracy: 0.9609\n",
      "Epoch [2/12], Step [90/375], Loss: 0.0773\n",
      "Epoch [2/12], Step [90/375], Accuracy: 0.9766\n",
      "Epoch [2/12], Step [100/375], Loss: 0.0542\n",
      "Epoch [2/12], Step [100/375], Accuracy: 0.9922\n",
      "Epoch [2/12], Step [110/375], Loss: 0.2248\n",
      "Epoch [2/12], Step [110/375], Accuracy: 0.9453\n",
      "Epoch [2/12], Step [120/375], Loss: 0.0876\n",
      "Epoch [2/12], Step [120/375], Accuracy: 0.9766\n",
      "Epoch [2/12], Step [130/375], Loss: 0.0637\n",
      "Epoch [2/12], Step [130/375], Accuracy: 0.9844\n",
      "Epoch [2/12], Step [140/375], Loss: 0.0563\n",
      "Epoch [2/12], Step [140/375], Accuracy: 0.9688\n",
      "Epoch [2/12], Step [150/375], Loss: 0.0572\n",
      "Epoch [2/12], Step [150/375], Accuracy: 0.9844\n",
      "Epoch [2/12], Step [160/375], Loss: 0.0490\n",
      "Epoch [2/12], Step [160/375], Accuracy: 0.9844\n",
      "Epoch [2/12], Step [170/375], Loss: 0.0224\n",
      "Epoch [2/12], Step [170/375], Accuracy: 1.0000\n",
      "Epoch [2/12], Step [180/375], Loss: 0.1781\n",
      "Epoch [2/12], Step [180/375], Accuracy: 0.9609\n",
      "Epoch [2/12], Step [190/375], Loss: 0.0895\n",
      "Epoch [2/12], Step [190/375], Accuracy: 0.9766\n",
      "Epoch [2/12], Step [200/375], Loss: 0.0795\n",
      "Epoch [2/12], Step [200/375], Accuracy: 0.9766\n",
      "Epoch [2/12], Step [210/375], Loss: 0.0608\n",
      "Epoch [2/12], Step [210/375], Accuracy: 0.9766\n",
      "Epoch [2/12], Step [220/375], Loss: 0.0813\n",
      "Epoch [2/12], Step [220/375], Accuracy: 0.9844\n",
      "Epoch [2/12], Step [230/375], Loss: 0.0672\n",
      "Epoch [2/12], Step [230/375], Accuracy: 0.9766\n",
      "Epoch [2/12], Step [240/375], Loss: 0.0280\n",
      "Epoch [2/12], Step [240/375], Accuracy: 0.9922\n",
      "Epoch [2/12], Step [250/375], Loss: 0.1296\n",
      "Epoch [2/12], Step [250/375], Accuracy: 0.9609\n",
      "Epoch [2/12], Step [260/375], Loss: 0.0888\n",
      "Epoch [2/12], Step [260/375], Accuracy: 0.9688\n",
      "Epoch [2/12], Step [270/375], Loss: 0.0884\n",
      "Epoch [2/12], Step [270/375], Accuracy: 0.9766\n",
      "Epoch [2/12], Step [280/375], Loss: 0.0255\n",
      "Epoch [2/12], Step [280/375], Accuracy: 0.9922\n",
      "Epoch [2/12], Step [290/375], Loss: 0.0383\n",
      "Epoch [2/12], Step [290/375], Accuracy: 0.9922\n",
      "Epoch [2/12], Step [300/375], Loss: 0.1186\n",
      "Epoch [2/12], Step [300/375], Accuracy: 0.9688\n",
      "Epoch [2/12], Step [310/375], Loss: 0.0077\n",
      "Epoch [2/12], Step [310/375], Accuracy: 1.0000\n",
      "Epoch [2/12], Step [320/375], Loss: 0.1711\n",
      "Epoch [2/12], Step [320/375], Accuracy: 0.9453\n",
      "Epoch [2/12], Step [330/375], Loss: 0.0946\n",
      "Epoch [2/12], Step [330/375], Accuracy: 0.9609\n",
      "Epoch [2/12], Step [340/375], Loss: 0.1008\n",
      "Epoch [2/12], Step [340/375], Accuracy: 0.9688\n",
      "Epoch [2/12], Step [350/375], Loss: 0.1261\n",
      "Epoch [2/12], Step [350/375], Accuracy: 0.9922\n",
      "Epoch [2/12], Step [360/375], Loss: 0.1858\n",
      "Epoch [2/12], Step [360/375], Accuracy: 0.9688\n",
      "Epoch [2/12], Step [370/375], Loss: 0.1873\n",
      "Epoch [2/12], Step [370/375], Accuracy: 0.9531\n",
      "Epoch [2/12], Step [0/94], Loss: 0.0040 Accuracy: 1.0000\n",
      "Epoch [2/12], Step [10/94], Loss: 0.0374 Accuracy: 0.9844\n",
      "Epoch [2/12], Step [20/94], Loss: 0.0354 Accuracy: 0.9844\n",
      "Epoch [2/12], Step [30/94], Loss: 0.0126 Accuracy: 1.0000\n",
      "Epoch [2/12], Step [40/94], Loss: 0.0819 Accuracy: 0.9688\n",
      "Epoch [2/12], Step [50/94], Loss: 0.0317 Accuracy: 0.9922\n",
      "Epoch [2/12], Step [60/94], Loss: 0.0677 Accuracy: 0.9766\n",
      "Epoch [2/12], Step [70/94], Loss: 0.0550 Accuracy: 0.9844\n",
      "Epoch [2/12], Step [80/94], Loss: 0.0057 Accuracy: 1.0000\n",
      "Epoch [2/12], Step [90/94], Loss: 0.1169 Accuracy: 0.9844\n",
      "Epoch [3/12], Step [0/375], Loss: 0.0497\n",
      "Epoch [3/12], Step [0/375], Accuracy: 0.9844\n",
      "Epoch [3/12], Step [10/375], Loss: 0.0686\n",
      "Epoch [3/12], Step [10/375], Accuracy: 0.9688\n",
      "Epoch [3/12], Step [20/375], Loss: 0.0287\n",
      "Epoch [3/12], Step [20/375], Accuracy: 0.9922\n",
      "Epoch [3/12], Step [30/375], Loss: 0.0088\n",
      "Epoch [3/12], Step [30/375], Accuracy: 1.0000\n",
      "Epoch [3/12], Step [40/375], Loss: 0.0938\n",
      "Epoch [3/12], Step [40/375], Accuracy: 0.9609\n",
      "Epoch [3/12], Step [50/375], Loss: 0.0866\n",
      "Epoch [3/12], Step [50/375], Accuracy: 0.9609\n",
      "Epoch [3/12], Step [60/375], Loss: 0.0994\n",
      "Epoch [3/12], Step [60/375], Accuracy: 0.9766\n",
      "Epoch [3/12], Step [70/375], Loss: 0.0210\n",
      "Epoch [3/12], Step [70/375], Accuracy: 0.9922\n",
      "Epoch [3/12], Step [80/375], Loss: 0.0661\n",
      "Epoch [3/12], Step [80/375], Accuracy: 0.9688\n",
      "Epoch [3/12], Step [90/375], Loss: 0.0358\n",
      "Epoch [3/12], Step [90/375], Accuracy: 0.9922\n",
      "Epoch [3/12], Step [100/375], Loss: 0.0910\n",
      "Epoch [3/12], Step [100/375], Accuracy: 0.9688\n",
      "Epoch [3/12], Step [110/375], Loss: 0.1065\n",
      "Epoch [3/12], Step [110/375], Accuracy: 0.9609\n",
      "Epoch [3/12], Step [120/375], Loss: 0.1367\n",
      "Epoch [3/12], Step [120/375], Accuracy: 0.9688\n",
      "Epoch [3/12], Step [130/375], Loss: 0.0330\n",
      "Epoch [3/12], Step [130/375], Accuracy: 0.9922\n",
      "Epoch [3/12], Step [140/375], Loss: 0.0271\n",
      "Epoch [3/12], Step [140/375], Accuracy: 0.9922\n",
      "Epoch [3/12], Step [150/375], Loss: 0.0878\n",
      "Epoch [3/12], Step [150/375], Accuracy: 0.9688\n",
      "Epoch [3/12], Step [160/375], Loss: 0.0158\n",
      "Epoch [3/12], Step [160/375], Accuracy: 1.0000\n",
      "Epoch [3/12], Step [170/375], Loss: 0.0424\n",
      "Epoch [3/12], Step [170/375], Accuracy: 0.9844\n",
      "Epoch [3/12], Step [180/375], Loss: 0.0341\n",
      "Epoch [3/12], Step [180/375], Accuracy: 0.9922\n",
      "Epoch [3/12], Step [190/375], Loss: 0.0979\n",
      "Epoch [3/12], Step [190/375], Accuracy: 0.9766\n",
      "Epoch [3/12], Step [200/375], Loss: 0.0522\n",
      "Epoch [3/12], Step [200/375], Accuracy: 0.9844\n",
      "Epoch [3/12], Step [210/375], Loss: 0.0924\n",
      "Epoch [3/12], Step [210/375], Accuracy: 0.9609\n",
      "Epoch [3/12], Step [220/375], Loss: 0.0551\n",
      "Epoch [3/12], Step [220/375], Accuracy: 0.9922\n",
      "Epoch [3/12], Step [230/375], Loss: 0.0230\n",
      "Epoch [3/12], Step [230/375], Accuracy: 0.9922\n",
      "Epoch [3/12], Step [240/375], Loss: 0.0687\n",
      "Epoch [3/12], Step [240/375], Accuracy: 0.9766\n",
      "Epoch [3/12], Step [250/375], Loss: 0.0328\n",
      "Epoch [3/12], Step [250/375], Accuracy: 0.9922\n",
      "Epoch [3/12], Step [260/375], Loss: 0.0154\n",
      "Epoch [3/12], Step [260/375], Accuracy: 0.9922\n",
      "Epoch [3/12], Step [270/375], Loss: 0.0089\n",
      "Epoch [3/12], Step [270/375], Accuracy: 1.0000\n",
      "Epoch [3/12], Step [280/375], Loss: 0.0350\n",
      "Epoch [3/12], Step [280/375], Accuracy: 0.9766\n",
      "Epoch [3/12], Step [290/375], Loss: 0.0736\n",
      "Epoch [3/12], Step [290/375], Accuracy: 0.9844\n",
      "Epoch [3/12], Step [300/375], Loss: 0.1018\n",
      "Epoch [3/12], Step [300/375], Accuracy: 0.9844\n",
      "Epoch [3/12], Step [310/375], Loss: 0.2162\n",
      "Epoch [3/12], Step [310/375], Accuracy: 0.9688\n",
      "Epoch [3/12], Step [320/375], Loss: 0.0632\n",
      "Epoch [3/12], Step [320/375], Accuracy: 0.9688\n",
      "Epoch [3/12], Step [330/375], Loss: 0.0360\n",
      "Epoch [3/12], Step [330/375], Accuracy: 0.9844\n",
      "Epoch [3/12], Step [340/375], Loss: 0.0875\n",
      "Epoch [3/12], Step [340/375], Accuracy: 0.9844\n",
      "Epoch [3/12], Step [350/375], Loss: 0.0262\n",
      "Epoch [3/12], Step [350/375], Accuracy: 0.9922\n",
      "Epoch [3/12], Step [360/375], Loss: 0.0525\n",
      "Epoch [3/12], Step [360/375], Accuracy: 0.9844\n",
      "Epoch [3/12], Step [370/375], Loss: 0.0457\n",
      "Epoch [3/12], Step [370/375], Accuracy: 0.9922\n",
      "Epoch [3/12], Step [0/94], Loss: 0.0148 Accuracy: 0.9922\n",
      "Epoch [3/12], Step [10/94], Loss: 0.0431 Accuracy: 0.9922\n",
      "Epoch [3/12], Step [20/94], Loss: 0.0666 Accuracy: 0.9766\n",
      "Epoch [3/12], Step [30/94], Loss: 0.0585 Accuracy: 0.9688\n",
      "Epoch [3/12], Step [40/94], Loss: 0.0478 Accuracy: 0.9766\n",
      "Epoch [3/12], Step [50/94], Loss: 0.0415 Accuracy: 0.9844\n",
      "Epoch [3/12], Step [60/94], Loss: 0.0366 Accuracy: 0.9922\n",
      "Epoch [3/12], Step [70/94], Loss: 0.0689 Accuracy: 0.9844\n",
      "Epoch [3/12], Step [80/94], Loss: 0.0517 Accuracy: 0.9844\n",
      "Epoch [3/12], Step [90/94], Loss: 0.1083 Accuracy: 0.9844\n",
      "Epoch [4/12], Step [0/375], Loss: 0.1978\n",
      "Epoch [4/12], Step [0/375], Accuracy: 0.9766\n",
      "Epoch [4/12], Step [10/375], Loss: 0.0737\n",
      "Epoch [4/12], Step [10/375], Accuracy: 0.9688\n",
      "Epoch [4/12], Step [20/375], Loss: 0.0410\n",
      "Epoch [4/12], Step [20/375], Accuracy: 0.9922\n",
      "Epoch [4/12], Step [30/375], Loss: 0.0816\n",
      "Epoch [4/12], Step [30/375], Accuracy: 0.9766\n",
      "Epoch [4/12], Step [40/375], Loss: 0.0981\n",
      "Epoch [4/12], Step [40/375], Accuracy: 0.9766\n",
      "Epoch [4/12], Step [50/375], Loss: 0.0030\n",
      "Epoch [4/12], Step [50/375], Accuracy: 1.0000\n",
      "Epoch [4/12], Step [60/375], Loss: 0.0774\n",
      "Epoch [4/12], Step [60/375], Accuracy: 0.9688\n",
      "Epoch [4/12], Step [70/375], Loss: 0.0800\n",
      "Epoch [4/12], Step [70/375], Accuracy: 0.9766\n",
      "Epoch [4/12], Step [80/375], Loss: 0.0649\n",
      "Epoch [4/12], Step [80/375], Accuracy: 0.9766\n",
      "Epoch [4/12], Step [90/375], Loss: 0.0428\n",
      "Epoch [4/12], Step [90/375], Accuracy: 0.9844\n",
      "Epoch [4/12], Step [100/375], Loss: 0.1741\n",
      "Epoch [4/12], Step [100/375], Accuracy: 0.9531\n",
      "Epoch [4/12], Step [110/375], Loss: 0.0455\n",
      "Epoch [4/12], Step [110/375], Accuracy: 0.9922\n",
      "Epoch [4/12], Step [120/375], Loss: 0.0292\n",
      "Epoch [4/12], Step [120/375], Accuracy: 0.9844\n",
      "Epoch [4/12], Step [130/375], Loss: 0.1960\n",
      "Epoch [4/12], Step [130/375], Accuracy: 0.9688\n",
      "Epoch [4/12], Step [140/375], Loss: 0.0549\n",
      "Epoch [4/12], Step [140/375], Accuracy: 0.9688\n",
      "Epoch [4/12], Step [150/375], Loss: 0.0832\n",
      "Epoch [4/12], Step [150/375], Accuracy: 0.9844\n",
      "Epoch [4/12], Step [160/375], Loss: 0.0229\n",
      "Epoch [4/12], Step [160/375], Accuracy: 0.9844\n",
      "Epoch [4/12], Step [170/375], Loss: 0.1039\n",
      "Epoch [4/12], Step [170/375], Accuracy: 0.9609\n",
      "Epoch [4/12], Step [180/375], Loss: 0.0190\n",
      "Epoch [4/12], Step [180/375], Accuracy: 0.9922\n",
      "Epoch [4/12], Step [190/375], Loss: 0.0289\n",
      "Epoch [4/12], Step [190/375], Accuracy: 0.9922\n",
      "Epoch [4/12], Step [200/375], Loss: 0.0330\n",
      "Epoch [4/12], Step [200/375], Accuracy: 0.9844\n",
      "Epoch [4/12], Step [210/375], Loss: 0.0251\n",
      "Epoch [4/12], Step [210/375], Accuracy: 0.9922\n",
      "Epoch [4/12], Step [220/375], Loss: 0.0818\n",
      "Epoch [4/12], Step [220/375], Accuracy: 0.9766\n",
      "Epoch [4/12], Step [230/375], Loss: 0.0972\n",
      "Epoch [4/12], Step [230/375], Accuracy: 0.9766\n",
      "Epoch [4/12], Step [240/375], Loss: 0.0691\n",
      "Epoch [4/12], Step [240/375], Accuracy: 0.9766\n",
      "Epoch [4/12], Step [250/375], Loss: 0.0658\n",
      "Epoch [4/12], Step [250/375], Accuracy: 0.9766\n",
      "Epoch [4/12], Step [260/375], Loss: 0.1370\n",
      "Epoch [4/12], Step [260/375], Accuracy: 0.9688\n",
      "Epoch [4/12], Step [270/375], Loss: 0.0340\n",
      "Epoch [4/12], Step [270/375], Accuracy: 0.9922\n",
      "Epoch [4/12], Step [280/375], Loss: 0.0999\n",
      "Epoch [4/12], Step [280/375], Accuracy: 0.9766\n",
      "Epoch [4/12], Step [290/375], Loss: 0.0812\n",
      "Epoch [4/12], Step [290/375], Accuracy: 0.9766\n",
      "Epoch [4/12], Step [300/375], Loss: 0.0201\n",
      "Epoch [4/12], Step [300/375], Accuracy: 0.9922\n",
      "Epoch [4/12], Step [310/375], Loss: 0.0388\n",
      "Epoch [4/12], Step [310/375], Accuracy: 0.9844\n",
      "Epoch [4/12], Step [320/375], Loss: 0.0147\n",
      "Epoch [4/12], Step [320/375], Accuracy: 1.0000\n",
      "Epoch [4/12], Step [330/375], Loss: 0.0281\n",
      "Epoch [4/12], Step [330/375], Accuracy: 0.9922\n",
      "Epoch [4/12], Step [340/375], Loss: 0.0329\n",
      "Epoch [4/12], Step [340/375], Accuracy: 0.9922\n",
      "Epoch [4/12], Step [350/375], Loss: 0.0269\n",
      "Epoch [4/12], Step [350/375], Accuracy: 1.0000\n",
      "Epoch [4/12], Step [360/375], Loss: 0.0541\n",
      "Epoch [4/12], Step [360/375], Accuracy: 0.9766\n",
      "Epoch [4/12], Step [370/375], Loss: 0.0833\n",
      "Epoch [4/12], Step [370/375], Accuracy: 0.9688\n",
      "Epoch [4/12], Step [0/94], Loss: 0.0020 Accuracy: 1.0000\n",
      "Epoch [4/12], Step [10/94], Loss: 0.0249 Accuracy: 0.9844\n",
      "Epoch [4/12], Step [20/94], Loss: 0.0363 Accuracy: 0.9844\n",
      "Epoch [4/12], Step [30/94], Loss: 0.0131 Accuracy: 1.0000\n",
      "Epoch [4/12], Step [40/94], Loss: 0.1443 Accuracy: 0.9766\n",
      "Epoch [4/12], Step [50/94], Loss: 0.0078 Accuracy: 1.0000\n",
      "Epoch [4/12], Step [60/94], Loss: 0.0527 Accuracy: 0.9922\n",
      "Epoch [4/12], Step [70/94], Loss: 0.0710 Accuracy: 0.9844\n",
      "Epoch [4/12], Step [80/94], Loss: 0.0128 Accuracy: 1.0000\n",
      "Epoch [4/12], Step [90/94], Loss: 0.0106 Accuracy: 1.0000\n",
      "Epoch [5/12], Step [0/375], Loss: 0.0217\n",
      "Epoch [5/12], Step [0/375], Accuracy: 0.9922\n",
      "Epoch [5/12], Step [10/375], Loss: 0.1716\n",
      "Epoch [5/12], Step [10/375], Accuracy: 0.9766\n",
      "Epoch [5/12], Step [20/375], Loss: 0.0375\n",
      "Epoch [5/12], Step [20/375], Accuracy: 0.9844\n",
      "Epoch [5/12], Step [30/375], Loss: 0.0758\n",
      "Epoch [5/12], Step [30/375], Accuracy: 0.9766\n",
      "Epoch [5/12], Step [40/375], Loss: 0.1020\n",
      "Epoch [5/12], Step [40/375], Accuracy: 0.9766\n",
      "Epoch [5/12], Step [50/375], Loss: 0.0791\n",
      "Epoch [5/12], Step [50/375], Accuracy: 0.9688\n",
      "Epoch [5/12], Step [60/375], Loss: 0.0948\n",
      "Epoch [5/12], Step [60/375], Accuracy: 0.9688\n",
      "Epoch [5/12], Step [70/375], Loss: 0.0511\n",
      "Epoch [5/12], Step [70/375], Accuracy: 0.9844\n",
      "Epoch [5/12], Step [80/375], Loss: 0.1192\n",
      "Epoch [5/12], Step [80/375], Accuracy: 0.9688\n",
      "Epoch [5/12], Step [90/375], Loss: 0.0134\n",
      "Epoch [5/12], Step [90/375], Accuracy: 1.0000\n",
      "Epoch [5/12], Step [100/375], Loss: 0.0497\n",
      "Epoch [5/12], Step [100/375], Accuracy: 0.9844\n",
      "Epoch [5/12], Step [110/375], Loss: 0.1909\n",
      "Epoch [5/12], Step [110/375], Accuracy: 0.9844\n",
      "Epoch [5/12], Step [120/375], Loss: 0.0560\n",
      "Epoch [5/12], Step [120/375], Accuracy: 0.9844\n",
      "Epoch [5/12], Step [130/375], Loss: 0.0547\n",
      "Epoch [5/12], Step [130/375], Accuracy: 0.9766\n",
      "Epoch [5/12], Step [140/375], Loss: 0.0143\n",
      "Epoch [5/12], Step [140/375], Accuracy: 1.0000\n",
      "Epoch [5/12], Step [150/375], Loss: 0.0256\n",
      "Epoch [5/12], Step [150/375], Accuracy: 0.9922\n",
      "Epoch [5/12], Step [160/375], Loss: 0.1057\n",
      "Epoch [5/12], Step [160/375], Accuracy: 0.9844\n",
      "Epoch [5/12], Step [170/375], Loss: 0.1835\n",
      "Epoch [5/12], Step [170/375], Accuracy: 0.9531\n",
      "Epoch [5/12], Step [180/375], Loss: 0.0570\n",
      "Epoch [5/12], Step [180/375], Accuracy: 0.9766\n",
      "Epoch [5/12], Step [190/375], Loss: 0.1381\n",
      "Epoch [5/12], Step [190/375], Accuracy: 0.9531\n",
      "Epoch [5/12], Step [200/375], Loss: 0.0134\n",
      "Epoch [5/12], Step [200/375], Accuracy: 1.0000\n",
      "Epoch [5/12], Step [210/375], Loss: 0.0085\n",
      "Epoch [5/12], Step [210/375], Accuracy: 1.0000\n",
      "Epoch [5/12], Step [220/375], Loss: 0.0659\n",
      "Epoch [5/12], Step [220/375], Accuracy: 0.9766\n",
      "Epoch [5/12], Step [230/375], Loss: 0.0256\n",
      "Epoch [5/12], Step [230/375], Accuracy: 0.9922\n",
      "Epoch [5/12], Step [240/375], Loss: 0.0326\n",
      "Epoch [5/12], Step [240/375], Accuracy: 0.9922\n",
      "Epoch [5/12], Step [250/375], Loss: 0.1126\n",
      "Epoch [5/12], Step [250/375], Accuracy: 0.9531\n",
      "Epoch [5/12], Step [260/375], Loss: 0.0325\n",
      "Epoch [5/12], Step [260/375], Accuracy: 0.9844\n",
      "Epoch [5/12], Step [270/375], Loss: 0.0369\n",
      "Epoch [5/12], Step [270/375], Accuracy: 0.9922\n",
      "Epoch [5/12], Step [280/375], Loss: 0.0095\n",
      "Epoch [5/12], Step [280/375], Accuracy: 1.0000\n",
      "Epoch [5/12], Step [290/375], Loss: 0.1345\n",
      "Epoch [5/12], Step [290/375], Accuracy: 0.9531\n",
      "Epoch [5/12], Step [300/375], Loss: 0.0756\n",
      "Epoch [5/12], Step [300/375], Accuracy: 0.9844\n",
      "Epoch [5/12], Step [310/375], Loss: 0.0236\n",
      "Epoch [5/12], Step [310/375], Accuracy: 1.0000\n",
      "Epoch [5/12], Step [320/375], Loss: 0.0977\n",
      "Epoch [5/12], Step [320/375], Accuracy: 0.9688\n",
      "Epoch [5/12], Step [330/375], Loss: 0.0509\n",
      "Epoch [5/12], Step [330/375], Accuracy: 0.9688\n",
      "Epoch [5/12], Step [340/375], Loss: 0.0661\n",
      "Epoch [5/12], Step [340/375], Accuracy: 0.9766\n",
      "Epoch [5/12], Step [350/375], Loss: 0.2524\n",
      "Epoch [5/12], Step [350/375], Accuracy: 0.9453\n",
      "Epoch [5/12], Step [360/375], Loss: 0.0652\n",
      "Epoch [5/12], Step [360/375], Accuracy: 0.9844\n",
      "Epoch [5/12], Step [370/375], Loss: 0.0353\n",
      "Epoch [5/12], Step [370/375], Accuracy: 0.9922\n",
      "Epoch [5/12], Step [0/94], Loss: 0.0068 Accuracy: 1.0000\n",
      "Epoch [5/12], Step [10/94], Loss: 0.0679 Accuracy: 0.9688\n",
      "Epoch [5/12], Step [20/94], Loss: 0.0826 Accuracy: 0.9766\n",
      "Epoch [5/12], Step [30/94], Loss: 0.0236 Accuracy: 1.0000\n",
      "Epoch [5/12], Step [40/94], Loss: 0.0615 Accuracy: 0.9844\n",
      "Epoch [5/12], Step [50/94], Loss: 0.0638 Accuracy: 0.9766\n",
      "Epoch [5/12], Step [60/94], Loss: 0.1675 Accuracy: 0.9688\n",
      "Epoch [5/12], Step [70/94], Loss: 0.0783 Accuracy: 0.9766\n",
      "Epoch [5/12], Step [80/94], Loss: 0.0283 Accuracy: 0.9844\n",
      "Epoch [5/12], Step [90/94], Loss: 0.0232 Accuracy: 0.9922\n",
      "Epoch [6/12], Step [0/375], Loss: 0.0648\n",
      "Epoch [6/12], Step [0/375], Accuracy: 0.9766\n",
      "Epoch [6/12], Step [10/375], Loss: 0.0415\n",
      "Epoch [6/12], Step [10/375], Accuracy: 0.9844\n",
      "Epoch [6/12], Step [20/375], Loss: 0.0350\n",
      "Epoch [6/12], Step [20/375], Accuracy: 0.9844\n",
      "Epoch [6/12], Step [30/375], Loss: 0.0078\n",
      "Epoch [6/12], Step [30/375], Accuracy: 1.0000\n",
      "Epoch [6/12], Step [40/375], Loss: 0.1202\n",
      "Epoch [6/12], Step [40/375], Accuracy: 0.9609\n",
      "Epoch [6/12], Step [50/375], Loss: 0.0395\n",
      "Epoch [6/12], Step [50/375], Accuracy: 0.9844\n",
      "Epoch [6/12], Step [60/375], Loss: 0.0657\n",
      "Epoch [6/12], Step [60/375], Accuracy: 0.9844\n",
      "Epoch [6/12], Step [70/375], Loss: 0.0592\n",
      "Epoch [6/12], Step [70/375], Accuracy: 0.9922\n",
      "Epoch [6/12], Step [80/375], Loss: 0.1544\n",
      "Epoch [6/12], Step [80/375], Accuracy: 0.9766\n",
      "Epoch [6/12], Step [90/375], Loss: 0.0299\n",
      "Epoch [6/12], Step [90/375], Accuracy: 0.9844\n",
      "Epoch [6/12], Step [100/375], Loss: 0.1515\n",
      "Epoch [6/12], Step [100/375], Accuracy: 0.9453\n",
      "Epoch [6/12], Step [110/375], Loss: 0.0186\n",
      "Epoch [6/12], Step [110/375], Accuracy: 0.9922\n",
      "Epoch [6/12], Step [120/375], Loss: 0.0246\n",
      "Epoch [6/12], Step [120/375], Accuracy: 1.0000\n",
      "Epoch [6/12], Step [130/375], Loss: 0.0678\n",
      "Epoch [6/12], Step [130/375], Accuracy: 0.9766\n",
      "Epoch [6/12], Step [140/375], Loss: 0.0215\n",
      "Epoch [6/12], Step [140/375], Accuracy: 0.9922\n",
      "Epoch [6/12], Step [150/375], Loss: 0.0072\n",
      "Epoch [6/12], Step [150/375], Accuracy: 1.0000\n",
      "Epoch [6/12], Step [160/375], Loss: 0.0723\n",
      "Epoch [6/12], Step [160/375], Accuracy: 0.9844\n",
      "Epoch [6/12], Step [170/375], Loss: 0.0386\n",
      "Epoch [6/12], Step [170/375], Accuracy: 0.9844\n",
      "Epoch [6/12], Step [180/375], Loss: 0.0343\n",
      "Epoch [6/12], Step [180/375], Accuracy: 0.9922\n",
      "Epoch [6/12], Step [190/375], Loss: 0.0302\n",
      "Epoch [6/12], Step [190/375], Accuracy: 0.9922\n",
      "Epoch [6/12], Step [200/375], Loss: 0.1955\n",
      "Epoch [6/12], Step [200/375], Accuracy: 0.9609\n",
      "Epoch [6/12], Step [210/375], Loss: 0.0591\n",
      "Epoch [6/12], Step [210/375], Accuracy: 0.9922\n",
      "Epoch [6/12], Step [220/375], Loss: 0.0143\n",
      "Epoch [6/12], Step [220/375], Accuracy: 0.9922\n",
      "Epoch [6/12], Step [230/375], Loss: 0.0071\n",
      "Epoch [6/12], Step [230/375], Accuracy: 1.0000\n",
      "Epoch [6/12], Step [240/375], Loss: 0.1069\n",
      "Epoch [6/12], Step [240/375], Accuracy: 0.9844\n",
      "Epoch [6/12], Step [250/375], Loss: 0.0498\n",
      "Epoch [6/12], Step [250/375], Accuracy: 0.9844\n",
      "Epoch [6/12], Step [260/375], Loss: 0.0396\n",
      "Epoch [6/12], Step [260/375], Accuracy: 0.9766\n",
      "Epoch [6/12], Step [270/375], Loss: 0.0734\n",
      "Epoch [6/12], Step [270/375], Accuracy: 0.9766\n",
      "Epoch [6/12], Step [280/375], Loss: 0.0377\n",
      "Epoch [6/12], Step [280/375], Accuracy: 0.9922\n",
      "Epoch [6/12], Step [290/375], Loss: 0.1185\n",
      "Epoch [6/12], Step [290/375], Accuracy: 0.9688\n",
      "Epoch [6/12], Step [300/375], Loss: 0.0703\n",
      "Epoch [6/12], Step [300/375], Accuracy: 0.9688\n",
      "Epoch [6/12], Step [310/375], Loss: 0.0272\n",
      "Epoch [6/12], Step [310/375], Accuracy: 0.9922\n",
      "Epoch [6/12], Step [320/375], Loss: 0.0205\n",
      "Epoch [6/12], Step [320/375], Accuracy: 0.9922\n",
      "Epoch [6/12], Step [330/375], Loss: 0.0712\n",
      "Epoch [6/12], Step [330/375], Accuracy: 0.9766\n",
      "Epoch [6/12], Step [340/375], Loss: 0.0796\n",
      "Epoch [6/12], Step [340/375], Accuracy: 0.9766\n",
      "Epoch [6/12], Step [350/375], Loss: 0.1118\n",
      "Epoch [6/12], Step [350/375], Accuracy: 0.9766\n",
      "Epoch [6/12], Step [360/375], Loss: 0.0825\n",
      "Epoch [6/12], Step [360/375], Accuracy: 0.9688\n",
      "Epoch [6/12], Step [370/375], Loss: 0.0570\n",
      "Epoch [6/12], Step [370/375], Accuracy: 0.9844\n",
      "Epoch [6/12], Step [0/94], Loss: 0.0019 Accuracy: 1.0000\n",
      "Epoch [6/12], Step [10/94], Loss: 0.0233 Accuracy: 0.9844\n",
      "Epoch [6/12], Step [20/94], Loss: 0.0074 Accuracy: 1.0000\n",
      "Epoch [6/12], Step [30/94], Loss: 0.0138 Accuracy: 0.9922\n",
      "Epoch [6/12], Step [40/94], Loss: 0.1383 Accuracy: 0.9688\n",
      "Epoch [6/12], Step [50/94], Loss: 0.0178 Accuracy: 0.9922\n",
      "Epoch [6/12], Step [60/94], Loss: 0.0395 Accuracy: 0.9922\n",
      "Epoch [6/12], Step [70/94], Loss: 0.0724 Accuracy: 0.9688\n",
      "Epoch [6/12], Step [80/94], Loss: 0.0419 Accuracy: 0.9922\n",
      "Epoch [6/12], Step [90/94], Loss: 0.0636 Accuracy: 0.9766\n",
      "Epoch [7/12], Step [0/375], Loss: 0.0083\n",
      "Epoch [7/12], Step [0/375], Accuracy: 1.0000\n",
      "Epoch [7/12], Step [10/375], Loss: 0.0316\n",
      "Epoch [7/12], Step [10/375], Accuracy: 0.9844\n",
      "Epoch [7/12], Step [20/375], Loss: 0.0193\n",
      "Epoch [7/12], Step [20/375], Accuracy: 0.9922\n",
      "Epoch [7/12], Step [30/375], Loss: 0.0175\n",
      "Epoch [7/12], Step [30/375], Accuracy: 0.9922\n",
      "Epoch [7/12], Step [40/375], Loss: 0.0574\n",
      "Epoch [7/12], Step [40/375], Accuracy: 0.9766\n",
      "Epoch [7/12], Step [50/375], Loss: 0.0555\n",
      "Epoch [7/12], Step [50/375], Accuracy: 0.9844\n",
      "Epoch [7/12], Step [60/375], Loss: 0.0583\n",
      "Epoch [7/12], Step [60/375], Accuracy: 0.9844\n",
      "Epoch [7/12], Step [70/375], Loss: 0.0093\n",
      "Epoch [7/12], Step [70/375], Accuracy: 1.0000\n",
      "Epoch [7/12], Step [80/375], Loss: 0.0226\n",
      "Epoch [7/12], Step [80/375], Accuracy: 0.9844\n",
      "Epoch [7/12], Step [90/375], Loss: 0.0363\n",
      "Epoch [7/12], Step [90/375], Accuracy: 0.9766\n",
      "Epoch [7/12], Step [100/375], Loss: 0.0267\n",
      "Epoch [7/12], Step [100/375], Accuracy: 0.9922\n",
      "Epoch [7/12], Step [110/375], Loss: 0.0228\n",
      "Epoch [7/12], Step [110/375], Accuracy: 1.0000\n",
      "Epoch [7/12], Step [120/375], Loss: 0.0295\n",
      "Epoch [7/12], Step [120/375], Accuracy: 0.9844\n",
      "Epoch [7/12], Step [130/375], Loss: 0.0062\n",
      "Epoch [7/12], Step [130/375], Accuracy: 1.0000\n",
      "Epoch [7/12], Step [140/375], Loss: 0.0630\n",
      "Epoch [7/12], Step [140/375], Accuracy: 0.9844\n",
      "Epoch [7/12], Step [150/375], Loss: 0.0205\n",
      "Epoch [7/12], Step [150/375], Accuracy: 0.9922\n",
      "Epoch [7/12], Step [160/375], Loss: 0.0083\n",
      "Epoch [7/12], Step [160/375], Accuracy: 1.0000\n",
      "Epoch [7/12], Step [170/375], Loss: 0.0746\n",
      "Epoch [7/12], Step [170/375], Accuracy: 0.9844\n",
      "Epoch [7/12], Step [180/375], Loss: 0.1101\n",
      "Epoch [7/12], Step [180/375], Accuracy: 0.9766\n",
      "Epoch [7/12], Step [190/375], Loss: 0.0541\n",
      "Epoch [7/12], Step [190/375], Accuracy: 0.9844\n",
      "Epoch [7/12], Step [200/375], Loss: 0.0060\n",
      "Epoch [7/12], Step [200/375], Accuracy: 1.0000\n",
      "Epoch [7/12], Step [210/375], Loss: 0.0921\n",
      "Epoch [7/12], Step [210/375], Accuracy: 0.9844\n",
      "Epoch [7/12], Step [220/375], Loss: 0.0238\n",
      "Epoch [7/12], Step [220/375], Accuracy: 0.9922\n",
      "Epoch [7/12], Step [230/375], Loss: 0.1609\n",
      "Epoch [7/12], Step [230/375], Accuracy: 0.9453\n",
      "Epoch [7/12], Step [240/375], Loss: 0.0806\n",
      "Epoch [7/12], Step [240/375], Accuracy: 0.9766\n",
      "Epoch [7/12], Step [250/375], Loss: 0.0341\n",
      "Epoch [7/12], Step [250/375], Accuracy: 0.9766\n",
      "Epoch [7/12], Step [260/375], Loss: 0.1014\n",
      "Epoch [7/12], Step [260/375], Accuracy: 0.9844\n",
      "Epoch [7/12], Step [270/375], Loss: 0.0860\n",
      "Epoch [7/12], Step [270/375], Accuracy: 0.9844\n",
      "Epoch [7/12], Step [280/375], Loss: 0.0605\n",
      "Epoch [7/12], Step [280/375], Accuracy: 0.9922\n",
      "Epoch [7/12], Step [290/375], Loss: 0.0457\n",
      "Epoch [7/12], Step [290/375], Accuracy: 0.9844\n",
      "Epoch [7/12], Step [300/375], Loss: 0.0202\n",
      "Epoch [7/12], Step [300/375], Accuracy: 0.9922\n",
      "Epoch [7/12], Step [310/375], Loss: 0.0545\n",
      "Epoch [7/12], Step [310/375], Accuracy: 0.9766\n",
      "Epoch [7/12], Step [320/375], Loss: 0.0084\n",
      "Epoch [7/12], Step [320/375], Accuracy: 1.0000\n",
      "Epoch [7/12], Step [330/375], Loss: 0.0446\n",
      "Epoch [7/12], Step [330/375], Accuracy: 0.9922\n",
      "Epoch [7/12], Step [340/375], Loss: 0.0673\n",
      "Epoch [7/12], Step [340/375], Accuracy: 0.9844\n",
      "Epoch [7/12], Step [350/375], Loss: 0.0091\n",
      "Epoch [7/12], Step [350/375], Accuracy: 1.0000\n",
      "Epoch [7/12], Step [360/375], Loss: 0.0916\n",
      "Epoch [7/12], Step [360/375], Accuracy: 0.9609\n",
      "Epoch [7/12], Step [370/375], Loss: 0.1044\n",
      "Epoch [7/12], Step [370/375], Accuracy: 0.9688\n",
      "Epoch [7/12], Step [0/94], Loss: 0.0009 Accuracy: 1.0000\n",
      "Epoch [7/12], Step [10/94], Loss: 0.0056 Accuracy: 1.0000\n",
      "Epoch [7/12], Step [20/94], Loss: 0.0710 Accuracy: 0.9766\n",
      "Epoch [7/12], Step [30/94], Loss: 0.0597 Accuracy: 0.9766\n",
      "Epoch [7/12], Step [40/94], Loss: 0.0518 Accuracy: 0.9766\n",
      "Epoch [7/12], Step [50/94], Loss: 0.0359 Accuracy: 0.9844\n",
      "Epoch [7/12], Step [60/94], Loss: 0.0282 Accuracy: 0.9922\n",
      "Epoch [7/12], Step [70/94], Loss: 0.1016 Accuracy: 0.9688\n",
      "Epoch [7/12], Step [80/94], Loss: 0.0338 Accuracy: 0.9844\n",
      "Epoch [7/12], Step [90/94], Loss: 0.0053 Accuracy: 1.0000\n",
      "Epoch [8/12], Step [0/375], Loss: 0.0298\n",
      "Epoch [8/12], Step [0/375], Accuracy: 0.9922\n",
      "Epoch [8/12], Step [10/375], Loss: 0.0387\n",
      "Epoch [8/12], Step [10/375], Accuracy: 0.9766\n",
      "Epoch [8/12], Step [20/375], Loss: 0.0879\n",
      "Epoch [8/12], Step [20/375], Accuracy: 0.9688\n",
      "Epoch [8/12], Step [30/375], Loss: 0.1228\n",
      "Epoch [8/12], Step [30/375], Accuracy: 0.9766\n",
      "Epoch [8/12], Step [40/375], Loss: 0.0040\n",
      "Epoch [8/12], Step [40/375], Accuracy: 1.0000\n",
      "Epoch [8/12], Step [50/375], Loss: 0.0655\n",
      "Epoch [8/12], Step [50/375], Accuracy: 0.9844\n",
      "Epoch [8/12], Step [60/375], Loss: 0.2141\n",
      "Epoch [8/12], Step [60/375], Accuracy: 0.9766\n",
      "Epoch [8/12], Step [70/375], Loss: 0.0399\n",
      "Epoch [8/12], Step [70/375], Accuracy: 0.9844\n",
      "Epoch [8/12], Step [80/375], Loss: 0.0387\n",
      "Epoch [8/12], Step [80/375], Accuracy: 0.9766\n",
      "Epoch [8/12], Step [90/375], Loss: 0.0614\n",
      "Epoch [8/12], Step [90/375], Accuracy: 0.9844\n",
      "Epoch [8/12], Step [100/375], Loss: 0.0714\n",
      "Epoch [8/12], Step [100/375], Accuracy: 0.9844\n",
      "Epoch [8/12], Step [110/375], Loss: 0.1291\n",
      "Epoch [8/12], Step [110/375], Accuracy: 0.9766\n",
      "Epoch [8/12], Step [120/375], Loss: 0.0082\n",
      "Epoch [8/12], Step [120/375], Accuracy: 1.0000\n",
      "Epoch [8/12], Step [130/375], Loss: 0.1619\n",
      "Epoch [8/12], Step [130/375], Accuracy: 0.9766\n",
      "Epoch [8/12], Step [140/375], Loss: 0.0126\n",
      "Epoch [8/12], Step [140/375], Accuracy: 1.0000\n",
      "Epoch [8/12], Step [150/375], Loss: 0.0120\n",
      "Epoch [8/12], Step [150/375], Accuracy: 0.9922\n",
      "Epoch [8/12], Step [160/375], Loss: 0.0202\n",
      "Epoch [8/12], Step [160/375], Accuracy: 1.0000\n",
      "Epoch [8/12], Step [170/375], Loss: 0.0237\n",
      "Epoch [8/12], Step [170/375], Accuracy: 0.9922\n",
      "Epoch [8/12], Step [180/375], Loss: 0.0374\n",
      "Epoch [8/12], Step [180/375], Accuracy: 0.9922\n",
      "Epoch [8/12], Step [190/375], Loss: 0.0533\n",
      "Epoch [8/12], Step [190/375], Accuracy: 0.9922\n",
      "Epoch [8/12], Step [200/375], Loss: 0.0177\n",
      "Epoch [8/12], Step [200/375], Accuracy: 0.9922\n",
      "Epoch [8/12], Step [210/375], Loss: 0.0253\n",
      "Epoch [8/12], Step [210/375], Accuracy: 0.9922\n",
      "Epoch [8/12], Step [220/375], Loss: 0.0621\n",
      "Epoch [8/12], Step [220/375], Accuracy: 0.9922\n",
      "Epoch [8/12], Step [230/375], Loss: 0.1244\n",
      "Epoch [8/12], Step [230/375], Accuracy: 0.9844\n",
      "Epoch [8/12], Step [240/375], Loss: 0.0712\n",
      "Epoch [8/12], Step [240/375], Accuracy: 0.9766\n",
      "Epoch [8/12], Step [250/375], Loss: 0.0055\n",
      "Epoch [8/12], Step [250/375], Accuracy: 1.0000\n",
      "Epoch [8/12], Step [260/375], Loss: 0.0660\n",
      "Epoch [8/12], Step [260/375], Accuracy: 0.9844\n",
      "Epoch [8/12], Step [270/375], Loss: 0.0556\n",
      "Epoch [8/12], Step [270/375], Accuracy: 0.9688\n",
      "Epoch [8/12], Step [280/375], Loss: 0.0273\n",
      "Epoch [8/12], Step [280/375], Accuracy: 0.9922\n",
      "Epoch [8/12], Step [290/375], Loss: 0.0455\n",
      "Epoch [8/12], Step [290/375], Accuracy: 0.9922\n",
      "Epoch [8/12], Step [300/375], Loss: 0.0197\n",
      "Epoch [8/12], Step [300/375], Accuracy: 0.9922\n",
      "Epoch [8/12], Step [310/375], Loss: 0.0665\n",
      "Epoch [8/12], Step [310/375], Accuracy: 0.9766\n",
      "Epoch [8/12], Step [320/375], Loss: 0.0478\n",
      "Epoch [8/12], Step [320/375], Accuracy: 0.9766\n",
      "Epoch [8/12], Step [330/375], Loss: 0.0292\n",
      "Epoch [8/12], Step [330/375], Accuracy: 0.9922\n",
      "Epoch [8/12], Step [340/375], Loss: 0.0767\n",
      "Epoch [8/12], Step [340/375], Accuracy: 0.9766\n",
      "Epoch [8/12], Step [350/375], Loss: 0.1317\n",
      "Epoch [8/12], Step [350/375], Accuracy: 0.9688\n",
      "Epoch [8/12], Step [360/375], Loss: 0.0584\n",
      "Epoch [8/12], Step [360/375], Accuracy: 0.9922\n",
      "Epoch [8/12], Step [370/375], Loss: 0.0235\n",
      "Epoch [8/12], Step [370/375], Accuracy: 0.9844\n",
      "Epoch [8/12], Step [0/94], Loss: 0.0006 Accuracy: 1.0000\n",
      "Epoch [8/12], Step [10/94], Loss: 0.0162 Accuracy: 0.9922\n",
      "Epoch [8/12], Step [20/94], Loss: 0.0157 Accuracy: 0.9922\n",
      "Epoch [8/12], Step [30/94], Loss: 0.0454 Accuracy: 0.9922\n",
      "Epoch [8/12], Step [40/94], Loss: 0.2510 Accuracy: 0.9688\n",
      "Epoch [8/12], Step [50/94], Loss: 0.0148 Accuracy: 0.9922\n",
      "Epoch [8/12], Step [60/94], Loss: 0.0422 Accuracy: 0.9922\n",
      "Epoch [8/12], Step [70/94], Loss: 0.1414 Accuracy: 0.9766\n",
      "Epoch [8/12], Step [80/94], Loss: 0.0401 Accuracy: 0.9844\n",
      "Epoch [8/12], Step [90/94], Loss: 0.0778 Accuracy: 0.9922\n",
      "Epoch [9/12], Step [0/375], Loss: 0.0903\n",
      "Epoch [9/12], Step [0/375], Accuracy: 0.9844\n",
      "Epoch [9/12], Step [10/375], Loss: 0.0066\n",
      "Epoch [9/12], Step [10/375], Accuracy: 1.0000\n",
      "Epoch [9/12], Step [20/375], Loss: 0.2211\n",
      "Epoch [9/12], Step [20/375], Accuracy: 0.9453\n",
      "Epoch [9/12], Step [30/375], Loss: 0.0162\n",
      "Epoch [9/12], Step [30/375], Accuracy: 0.9922\n",
      "Epoch [9/12], Step [40/375], Loss: 0.0891\n",
      "Epoch [9/12], Step [40/375], Accuracy: 0.9844\n",
      "Epoch [9/12], Step [50/375], Loss: 0.0088\n",
      "Epoch [9/12], Step [50/375], Accuracy: 1.0000\n",
      "Epoch [9/12], Step [60/375], Loss: 0.0180\n",
      "Epoch [9/12], Step [60/375], Accuracy: 0.9922\n",
      "Epoch [9/12], Step [70/375], Loss: 0.0223\n",
      "Epoch [9/12], Step [70/375], Accuracy: 0.9922\n",
      "Epoch [9/12], Step [80/375], Loss: 0.0268\n",
      "Epoch [9/12], Step [80/375], Accuracy: 0.9922\n",
      "Epoch [9/12], Step [90/375], Loss: 0.0891\n",
      "Epoch [9/12], Step [90/375], Accuracy: 0.9844\n",
      "Epoch [9/12], Step [100/375], Loss: 0.1202\n",
      "Epoch [9/12], Step [100/375], Accuracy: 0.9766\n",
      "Epoch [9/12], Step [110/375], Loss: 0.1287\n",
      "Epoch [9/12], Step [110/375], Accuracy: 0.9922\n",
      "Epoch [9/12], Step [120/375], Loss: 0.0292\n",
      "Epoch [9/12], Step [120/375], Accuracy: 0.9922\n",
      "Epoch [9/12], Step [130/375], Loss: 0.1169\n",
      "Epoch [9/12], Step [130/375], Accuracy: 0.9609\n",
      "Epoch [9/12], Step [140/375], Loss: 0.0150\n",
      "Epoch [9/12], Step [140/375], Accuracy: 1.0000\n",
      "Epoch [9/12], Step [150/375], Loss: 0.0661\n",
      "Epoch [9/12], Step [150/375], Accuracy: 0.9766\n",
      "Epoch [9/12], Step [160/375], Loss: 0.0985\n",
      "Epoch [9/12], Step [160/375], Accuracy: 0.9688\n",
      "Epoch [9/12], Step [170/375], Loss: 0.1080\n",
      "Epoch [9/12], Step [170/375], Accuracy: 0.9531\n",
      "Epoch [9/12], Step [180/375], Loss: 0.0074\n",
      "Epoch [9/12], Step [180/375], Accuracy: 1.0000\n",
      "Epoch [9/12], Step [190/375], Loss: 0.0276\n",
      "Epoch [9/12], Step [190/375], Accuracy: 0.9922\n",
      "Epoch [9/12], Step [200/375], Loss: 0.0346\n",
      "Epoch [9/12], Step [200/375], Accuracy: 0.9922\n",
      "Epoch [9/12], Step [210/375], Loss: 0.0512\n",
      "Epoch [9/12], Step [210/375], Accuracy: 0.9844\n",
      "Epoch [9/12], Step [220/375], Loss: 0.0549\n",
      "Epoch [9/12], Step [220/375], Accuracy: 0.9844\n",
      "Epoch [9/12], Step [230/375], Loss: 0.1657\n",
      "Epoch [9/12], Step [230/375], Accuracy: 0.9688\n",
      "Epoch [9/12], Step [240/375], Loss: 0.1854\n",
      "Epoch [9/12], Step [240/375], Accuracy: 0.9688\n",
      "Epoch [9/12], Step [250/375], Loss: 0.0351\n",
      "Epoch [9/12], Step [250/375], Accuracy: 0.9844\n",
      "Epoch [9/12], Step [260/375], Loss: 0.0129\n",
      "Epoch [9/12], Step [260/375], Accuracy: 1.0000\n",
      "Epoch [9/12], Step [270/375], Loss: 0.0854\n",
      "Epoch [9/12], Step [270/375], Accuracy: 0.9766\n",
      "Epoch [9/12], Step [280/375], Loss: 0.0613\n",
      "Epoch [9/12], Step [280/375], Accuracy: 0.9844\n",
      "Epoch [9/12], Step [290/375], Loss: 0.0496\n",
      "Epoch [9/12], Step [290/375], Accuracy: 0.9844\n",
      "Epoch [9/12], Step [300/375], Loss: 0.1032\n",
      "Epoch [9/12], Step [300/375], Accuracy: 0.9922\n",
      "Epoch [9/12], Step [310/375], Loss: 0.0527\n",
      "Epoch [9/12], Step [310/375], Accuracy: 0.9844\n",
      "Epoch [9/12], Step [320/375], Loss: 0.1289\n",
      "Epoch [9/12], Step [320/375], Accuracy: 0.9766\n",
      "Epoch [9/12], Step [330/375], Loss: 0.0231\n",
      "Epoch [9/12], Step [330/375], Accuracy: 1.0000\n",
      "Epoch [9/12], Step [340/375], Loss: 0.1362\n",
      "Epoch [9/12], Step [340/375], Accuracy: 0.9766\n",
      "Epoch [9/12], Step [350/375], Loss: 0.0519\n",
      "Epoch [9/12], Step [350/375], Accuracy: 0.9844\n",
      "Epoch [9/12], Step [360/375], Loss: 0.1395\n",
      "Epoch [9/12], Step [360/375], Accuracy: 0.9609\n",
      "Epoch [9/12], Step [370/375], Loss: 0.1018\n",
      "Epoch [9/12], Step [370/375], Accuracy: 0.9688\n",
      "Epoch [9/12], Step [0/94], Loss: 0.0055 Accuracy: 1.0000\n",
      "Epoch [9/12], Step [10/94], Loss: 0.0180 Accuracy: 0.9922\n",
      "Epoch [9/12], Step [20/94], Loss: 0.0153 Accuracy: 1.0000\n",
      "Epoch [9/12], Step [30/94], Loss: 0.0351 Accuracy: 0.9844\n",
      "Epoch [9/12], Step [40/94], Loss: 0.0677 Accuracy: 0.9766\n",
      "Epoch [9/12], Step [50/94], Loss: 0.0307 Accuracy: 0.9844\n",
      "Epoch [9/12], Step [60/94], Loss: 0.0640 Accuracy: 0.9844\n",
      "Epoch [9/12], Step [70/94], Loss: 0.0664 Accuracy: 0.9766\n",
      "Epoch [9/12], Step [80/94], Loss: 0.0324 Accuracy: 0.9844\n",
      "Epoch [9/12], Step [90/94], Loss: 0.0137 Accuracy: 1.0000\n",
      "Epoch [10/12], Step [0/375], Loss: 0.0624\n",
      "Epoch [10/12], Step [0/375], Accuracy: 0.9766\n",
      "Epoch [10/12], Step [10/375], Loss: 0.0263\n",
      "Epoch [10/12], Step [10/375], Accuracy: 0.9922\n",
      "Epoch [10/12], Step [20/375], Loss: 0.0542\n",
      "Epoch [10/12], Step [20/375], Accuracy: 0.9766\n",
      "Epoch [10/12], Step [30/375], Loss: 0.0335\n",
      "Epoch [10/12], Step [30/375], Accuracy: 0.9766\n",
      "Epoch [10/12], Step [40/375], Loss: 0.0542\n",
      "Epoch [10/12], Step [40/375], Accuracy: 0.9844\n",
      "Epoch [10/12], Step [50/375], Loss: 0.0420\n",
      "Epoch [10/12], Step [50/375], Accuracy: 0.9844\n",
      "Epoch [10/12], Step [60/375], Loss: 0.0112\n",
      "Epoch [10/12], Step [60/375], Accuracy: 1.0000\n",
      "Epoch [10/12], Step [70/375], Loss: 0.0554\n",
      "Epoch [10/12], Step [70/375], Accuracy: 0.9688\n",
      "Epoch [10/12], Step [80/375], Loss: 0.0210\n",
      "Epoch [10/12], Step [80/375], Accuracy: 0.9922\n",
      "Epoch [10/12], Step [90/375], Loss: 0.0556\n",
      "Epoch [10/12], Step [90/375], Accuracy: 0.9844\n",
      "Epoch [10/12], Step [100/375], Loss: 0.0280\n",
      "Epoch [10/12], Step [100/375], Accuracy: 0.9922\n",
      "Epoch [10/12], Step [110/375], Loss: 0.0555\n",
      "Epoch [10/12], Step [110/375], Accuracy: 0.9688\n",
      "Epoch [10/12], Step [120/375], Loss: 0.0185\n",
      "Epoch [10/12], Step [120/375], Accuracy: 0.9922\n",
      "Epoch [10/12], Step [130/375], Loss: 0.0357\n",
      "Epoch [10/12], Step [130/375], Accuracy: 0.9844\n",
      "Epoch [10/12], Step [140/375], Loss: 0.1116\n",
      "Epoch [10/12], Step [140/375], Accuracy: 0.9688\n",
      "Epoch [10/12], Step [150/375], Loss: 0.0823\n",
      "Epoch [10/12], Step [150/375], Accuracy: 0.9766\n",
      "Epoch [10/12], Step [160/375], Loss: 0.0188\n",
      "Epoch [10/12], Step [160/375], Accuracy: 0.9922\n",
      "Epoch [10/12], Step [170/375], Loss: 0.0799\n",
      "Epoch [10/12], Step [170/375], Accuracy: 0.9844\n",
      "Epoch [10/12], Step [180/375], Loss: 0.2038\n",
      "Epoch [10/12], Step [180/375], Accuracy: 0.9609\n",
      "Epoch [10/12], Step [190/375], Loss: 0.1294\n",
      "Epoch [10/12], Step [190/375], Accuracy: 0.9766\n",
      "Epoch [10/12], Step [200/375], Loss: 0.0325\n",
      "Epoch [10/12], Step [200/375], Accuracy: 0.9922\n",
      "Epoch [10/12], Step [210/375], Loss: 0.0770\n",
      "Epoch [10/12], Step [210/375], Accuracy: 0.9844\n",
      "Epoch [10/12], Step [220/375], Loss: 0.0265\n",
      "Epoch [10/12], Step [220/375], Accuracy: 0.9844\n",
      "Epoch [10/12], Step [230/375], Loss: 0.0808\n",
      "Epoch [10/12], Step [230/375], Accuracy: 0.9688\n",
      "Epoch [10/12], Step [240/375], Loss: 0.0096\n",
      "Epoch [10/12], Step [240/375], Accuracy: 0.9922\n",
      "Epoch [10/12], Step [250/375], Loss: 0.1627\n",
      "Epoch [10/12], Step [250/375], Accuracy: 0.9609\n",
      "Epoch [10/12], Step [260/375], Loss: 0.0468\n",
      "Epoch [10/12], Step [260/375], Accuracy: 0.9844\n",
      "Epoch [10/12], Step [270/375], Loss: 0.0040\n",
      "Epoch [10/12], Step [270/375], Accuracy: 1.0000\n",
      "Epoch [10/12], Step [280/375], Loss: 0.0278\n",
      "Epoch [10/12], Step [280/375], Accuracy: 0.9844\n",
      "Epoch [10/12], Step [290/375], Loss: 0.0634\n",
      "Epoch [10/12], Step [290/375], Accuracy: 0.9766\n",
      "Epoch [10/12], Step [300/375], Loss: 0.0994\n",
      "Epoch [10/12], Step [300/375], Accuracy: 0.9766\n",
      "Epoch [10/12], Step [310/375], Loss: 0.0318\n",
      "Epoch [10/12], Step [310/375], Accuracy: 0.9844\n",
      "Epoch [10/12], Step [320/375], Loss: 0.0087\n",
      "Epoch [10/12], Step [320/375], Accuracy: 1.0000\n",
      "Epoch [10/12], Step [330/375], Loss: 0.0163\n",
      "Epoch [10/12], Step [330/375], Accuracy: 0.9922\n",
      "Epoch [10/12], Step [340/375], Loss: 0.2202\n",
      "Epoch [10/12], Step [340/375], Accuracy: 0.9609\n",
      "Epoch [10/12], Step [350/375], Loss: 0.0486\n",
      "Epoch [10/12], Step [350/375], Accuracy: 0.9844\n",
      "Epoch [10/12], Step [360/375], Loss: 0.0199\n",
      "Epoch [10/12], Step [360/375], Accuracy: 0.9922\n",
      "Epoch [10/12], Step [370/375], Loss: 0.0346\n",
      "Epoch [10/12], Step [370/375], Accuracy: 0.9844\n",
      "Epoch [10/12], Step [0/94], Loss: 0.0009 Accuracy: 1.0000\n",
      "Epoch [10/12], Step [10/94], Loss: 0.0260 Accuracy: 0.9922\n",
      "Epoch [10/12], Step [20/94], Loss: 0.0516 Accuracy: 0.9922\n",
      "Epoch [10/12], Step [30/94], Loss: 0.0288 Accuracy: 0.9844\n",
      "Epoch [10/12], Step [40/94], Loss: 0.0288 Accuracy: 0.9922\n",
      "Epoch [10/12], Step [50/94], Loss: 0.0172 Accuracy: 0.9922\n",
      "Epoch [10/12], Step [60/94], Loss: 0.0160 Accuracy: 0.9922\n",
      "Epoch [10/12], Step [70/94], Loss: 0.0592 Accuracy: 0.9766\n",
      "Epoch [10/12], Step [80/94], Loss: 0.0424 Accuracy: 0.9766\n",
      "Epoch [10/12], Step [90/94], Loss: 0.0255 Accuracy: 0.9922\n",
      "Epoch [11/12], Step [0/375], Loss: 0.0138\n",
      "Epoch [11/12], Step [0/375], Accuracy: 0.9922\n",
      "Epoch [11/12], Step [10/375], Loss: 0.0023\n",
      "Epoch [11/12], Step [10/375], Accuracy: 1.0000\n",
      "Epoch [11/12], Step [20/375], Loss: 0.0619\n",
      "Epoch [11/12], Step [20/375], Accuracy: 0.9766\n",
      "Epoch [11/12], Step [30/375], Loss: 0.0144\n",
      "Epoch [11/12], Step [30/375], Accuracy: 0.9922\n",
      "Epoch [11/12], Step [40/375], Loss: 0.0264\n",
      "Epoch [11/12], Step [40/375], Accuracy: 0.9766\n",
      "Epoch [11/12], Step [50/375], Loss: 0.0577\n",
      "Epoch [11/12], Step [50/375], Accuracy: 0.9766\n",
      "Epoch [11/12], Step [60/375], Loss: 0.0097\n",
      "Epoch [11/12], Step [60/375], Accuracy: 0.9922\n",
      "Epoch [11/12], Step [70/375], Loss: 0.0438\n",
      "Epoch [11/12], Step [70/375], Accuracy: 0.9922\n",
      "Epoch [11/12], Step [80/375], Loss: 0.0236\n",
      "Epoch [11/12], Step [80/375], Accuracy: 0.9922\n",
      "Epoch [11/12], Step [90/375], Loss: 0.0437\n",
      "Epoch [11/12], Step [90/375], Accuracy: 0.9766\n",
      "Epoch [11/12], Step [100/375], Loss: 0.0323\n",
      "Epoch [11/12], Step [100/375], Accuracy: 0.9922\n",
      "Epoch [11/12], Step [110/375], Loss: 0.0799\n",
      "Epoch [11/12], Step [110/375], Accuracy: 0.9844\n",
      "Epoch [11/12], Step [120/375], Loss: 0.0171\n",
      "Epoch [11/12], Step [120/375], Accuracy: 0.9922\n",
      "Epoch [11/12], Step [130/375], Loss: 0.0026\n",
      "Epoch [11/12], Step [130/375], Accuracy: 1.0000\n",
      "Epoch [11/12], Step [140/375], Loss: 0.0091\n",
      "Epoch [11/12], Step [140/375], Accuracy: 1.0000\n",
      "Epoch [11/12], Step [150/375], Loss: 0.1816\n",
      "Epoch [11/12], Step [150/375], Accuracy: 0.9766\n",
      "Epoch [11/12], Step [160/375], Loss: 0.0099\n",
      "Epoch [11/12], Step [160/375], Accuracy: 1.0000\n",
      "Epoch [11/12], Step [170/375], Loss: 0.0171\n",
      "Epoch [11/12], Step [170/375], Accuracy: 0.9922\n",
      "Epoch [11/12], Step [180/375], Loss: 0.0178\n",
      "Epoch [11/12], Step [180/375], Accuracy: 0.9844\n",
      "Epoch [11/12], Step [190/375], Loss: 0.0435\n",
      "Epoch [11/12], Step [190/375], Accuracy: 0.9844\n",
      "Epoch [11/12], Step [200/375], Loss: 0.1479\n",
      "Epoch [11/12], Step [200/375], Accuracy: 0.9766\n",
      "Epoch [11/12], Step [210/375], Loss: 0.0256\n",
      "Epoch [11/12], Step [210/375], Accuracy: 0.9922\n",
      "Epoch [11/12], Step [220/375], Loss: 0.0904\n",
      "Epoch [11/12], Step [220/375], Accuracy: 0.9688\n",
      "Epoch [11/12], Step [230/375], Loss: 0.0749\n",
      "Epoch [11/12], Step [230/375], Accuracy: 0.9688\n",
      "Epoch [11/12], Step [240/375], Loss: 0.0781\n",
      "Epoch [11/12], Step [240/375], Accuracy: 0.9844\n",
      "Epoch [11/12], Step [250/375], Loss: 0.0658\n",
      "Epoch [11/12], Step [250/375], Accuracy: 0.9688\n",
      "Epoch [11/12], Step [260/375], Loss: 0.0006\n",
      "Epoch [11/12], Step [260/375], Accuracy: 1.0000\n",
      "Epoch [11/12], Step [270/375], Loss: 0.0364\n",
      "Epoch [11/12], Step [270/375], Accuracy: 0.9922\n",
      "Epoch [11/12], Step [280/375], Loss: 0.0123\n",
      "Epoch [11/12], Step [280/375], Accuracy: 1.0000\n",
      "Epoch [11/12], Step [290/375], Loss: 0.0142\n",
      "Epoch [11/12], Step [290/375], Accuracy: 0.9922\n",
      "Epoch [11/12], Step [300/375], Loss: 0.0703\n",
      "Epoch [11/12], Step [300/375], Accuracy: 0.9844\n",
      "Epoch [11/12], Step [310/375], Loss: 0.0632\n",
      "Epoch [11/12], Step [310/375], Accuracy: 0.9844\n",
      "Epoch [11/12], Step [320/375], Loss: 0.0386\n",
      "Epoch [11/12], Step [320/375], Accuracy: 0.9844\n",
      "Epoch [11/12], Step [330/375], Loss: 0.0262\n",
      "Epoch [11/12], Step [330/375], Accuracy: 0.9844\n",
      "Epoch [11/12], Step [340/375], Loss: 0.0280\n",
      "Epoch [11/12], Step [340/375], Accuracy: 0.9922\n",
      "Epoch [11/12], Step [350/375], Loss: 0.0029\n",
      "Epoch [11/12], Step [350/375], Accuracy: 1.0000\n",
      "Epoch [11/12], Step [360/375], Loss: 0.2608\n",
      "Epoch [11/12], Step [360/375], Accuracy: 0.9688\n",
      "Epoch [11/12], Step [370/375], Loss: 0.0579\n",
      "Epoch [11/12], Step [370/375], Accuracy: 0.9844\n",
      "Epoch [11/12], Step [0/94], Loss: 0.0167 Accuracy: 1.0000\n",
      "Epoch [11/12], Step [10/94], Loss: 0.0264 Accuracy: 0.9844\n",
      "Epoch [11/12], Step [20/94], Loss: 0.0389 Accuracy: 0.9844\n",
      "Epoch [11/12], Step [30/94], Loss: 0.0490 Accuracy: 0.9844\n",
      "Epoch [11/12], Step [40/94], Loss: 0.1883 Accuracy: 0.9609\n",
      "Epoch [11/12], Step [50/94], Loss: 0.0206 Accuracy: 0.9922\n",
      "Epoch [11/12], Step [60/94], Loss: 0.0446 Accuracy: 0.9922\n",
      "Epoch [11/12], Step [70/94], Loss: 0.0728 Accuracy: 0.9844\n",
      "Epoch [11/12], Step [80/94], Loss: 0.0669 Accuracy: 0.9844\n",
      "Epoch [11/12], Step [90/94], Loss: 0.0313 Accuracy: 0.9922\n",
      "Epoch [12/12], Step [0/375], Loss: 0.1162\n",
      "Epoch [12/12], Step [0/375], Accuracy: 0.9688\n",
      "Epoch [12/12], Step [10/375], Loss: 0.0104\n",
      "Epoch [12/12], Step [10/375], Accuracy: 1.0000\n",
      "Epoch [12/12], Step [20/375], Loss: 0.0261\n",
      "Epoch [12/12], Step [20/375], Accuracy: 0.9922\n",
      "Epoch [12/12], Step [30/375], Loss: 0.0068\n",
      "Epoch [12/12], Step [30/375], Accuracy: 1.0000\n",
      "Epoch [12/12], Step [40/375], Loss: 0.0440\n",
      "Epoch [12/12], Step [40/375], Accuracy: 0.9922\n",
      "Epoch [12/12], Step [50/375], Loss: 0.0165\n",
      "Epoch [12/12], Step [50/375], Accuracy: 1.0000\n",
      "Epoch [12/12], Step [60/375], Loss: 0.0061\n",
      "Epoch [12/12], Step [60/375], Accuracy: 1.0000\n",
      "Epoch [12/12], Step [70/375], Loss: 0.0057\n",
      "Epoch [12/12], Step [70/375], Accuracy: 1.0000\n",
      "Epoch [12/12], Step [80/375], Loss: 0.0264\n",
      "Epoch [12/12], Step [80/375], Accuracy: 0.9922\n",
      "Epoch [12/12], Step [90/375], Loss: 0.0545\n",
      "Epoch [12/12], Step [90/375], Accuracy: 0.9922\n",
      "Epoch [12/12], Step [100/375], Loss: 0.0653\n",
      "Epoch [12/12], Step [100/375], Accuracy: 0.9688\n",
      "Epoch [12/12], Step [110/375], Loss: 0.0355\n",
      "Epoch [12/12], Step [110/375], Accuracy: 0.9922\n",
      "Epoch [12/12], Step [120/375], Loss: 0.0652\n",
      "Epoch [12/12], Step [120/375], Accuracy: 0.9766\n",
      "Epoch [12/12], Step [130/375], Loss: 0.0031\n",
      "Epoch [12/12], Step [130/375], Accuracy: 1.0000\n",
      "Epoch [12/12], Step [140/375], Loss: 0.0245\n",
      "Epoch [12/12], Step [140/375], Accuracy: 0.9844\n",
      "Epoch [12/12], Step [150/375], Loss: 0.0453\n",
      "Epoch [12/12], Step [150/375], Accuracy: 0.9922\n",
      "Epoch [12/12], Step [160/375], Loss: 0.0086\n",
      "Epoch [12/12], Step [160/375], Accuracy: 1.0000\n",
      "Epoch [12/12], Step [170/375], Loss: 0.1134\n",
      "Epoch [12/12], Step [170/375], Accuracy: 0.9688\n",
      "Epoch [12/12], Step [180/375], Loss: 0.0461\n",
      "Epoch [12/12], Step [180/375], Accuracy: 0.9766\n",
      "Epoch [12/12], Step [190/375], Loss: 0.0458\n",
      "Epoch [12/12], Step [190/375], Accuracy: 0.9844\n",
      "Epoch [12/12], Step [200/375], Loss: 0.0517\n",
      "Epoch [12/12], Step [200/375], Accuracy: 0.9844\n",
      "Epoch [12/12], Step [210/375], Loss: 0.1116\n",
      "Epoch [12/12], Step [210/375], Accuracy: 0.9844\n",
      "Epoch [12/12], Step [220/375], Loss: 0.1458\n",
      "Epoch [12/12], Step [220/375], Accuracy: 0.9766\n",
      "Epoch [12/12], Step [230/375], Loss: 0.0230\n",
      "Epoch [12/12], Step [230/375], Accuracy: 0.9922\n",
      "Epoch [12/12], Step [240/375], Loss: 0.0231\n",
      "Epoch [12/12], Step [240/375], Accuracy: 0.9922\n",
      "Epoch [12/12], Step [250/375], Loss: 0.0754\n",
      "Epoch [12/12], Step [250/375], Accuracy: 0.9766\n",
      "Epoch [12/12], Step [260/375], Loss: 0.0218\n",
      "Epoch [12/12], Step [260/375], Accuracy: 0.9922\n",
      "Epoch [12/12], Step [270/375], Loss: 0.0104\n",
      "Epoch [12/12], Step [270/375], Accuracy: 1.0000\n",
      "Epoch [12/12], Step [280/375], Loss: 0.0126\n",
      "Epoch [12/12], Step [280/375], Accuracy: 0.9922\n",
      "Epoch [12/12], Step [290/375], Loss: 0.0123\n",
      "Epoch [12/12], Step [290/375], Accuracy: 1.0000\n",
      "Epoch [12/12], Step [300/375], Loss: 0.0817\n",
      "Epoch [12/12], Step [300/375], Accuracy: 0.9844\n",
      "Epoch [12/12], Step [310/375], Loss: 0.0520\n",
      "Epoch [12/12], Step [310/375], Accuracy: 0.9922\n",
      "Epoch [12/12], Step [320/375], Loss: 0.0258\n",
      "Epoch [12/12], Step [320/375], Accuracy: 0.9922\n",
      "Epoch [12/12], Step [330/375], Loss: 0.0129\n",
      "Epoch [12/12], Step [330/375], Accuracy: 1.0000\n",
      "Epoch [12/12], Step [340/375], Loss: 0.0072\n",
      "Epoch [12/12], Step [340/375], Accuracy: 1.0000\n",
      "Epoch [12/12], Step [350/375], Loss: 0.0395\n",
      "Epoch [12/12], Step [350/375], Accuracy: 0.9922\n",
      "Epoch [12/12], Step [360/375], Loss: 0.0050\n",
      "Epoch [12/12], Step [360/375], Accuracy: 1.0000\n",
      "Epoch [12/12], Step [370/375], Loss: 0.0840\n",
      "Epoch [12/12], Step [370/375], Accuracy: 0.9766\n",
      "Epoch [12/12], Step [0/94], Loss: 0.0019 Accuracy: 1.0000\n",
      "Epoch [12/12], Step [10/94], Loss: 0.0092 Accuracy: 1.0000\n",
      "Epoch [12/12], Step [20/94], Loss: 0.0296 Accuracy: 0.9844\n",
      "Epoch [12/12], Step [30/94], Loss: 0.0127 Accuracy: 1.0000\n",
      "Epoch [12/12], Step [40/94], Loss: 0.0488 Accuracy: 0.9844\n",
      "Epoch [12/12], Step [50/94], Loss: 0.0203 Accuracy: 0.9922\n",
      "Epoch [12/12], Step [60/94], Loss: 0.0171 Accuracy: 0.9922\n",
      "Epoch [12/12], Step [70/94], Loss: 0.1610 Accuracy: 0.9766\n",
      "Epoch [12/12], Step [80/94], Loss: 0.0309 Accuracy: 0.9766\n",
      "Epoch [12/12], Step [90/94], Loss: 0.0354 Accuracy: 0.9844\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward\n",
    "        logits = model(features)\n",
    "        loss = loss_fn(logits, targets)\n",
    "        acc = accuracy(logits, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "        # log training\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{NUM_EPOCHS}], Step [{batch_idx}/{NUM_BATCHES}], Loss: {loss.item():.4f}')\n",
    "            writer.add_scalar('Training Loss(Every 10 batch)', loss.item(), epoch * NUM_BATCHES + batch_idx)\n",
    "            print(f'Epoch [{epoch + 1}/{NUM_EPOCHS}], Step [{batch_idx}/{NUM_BATCHES}], Accuracy: {acc.item():.4f}')\n",
    "            writer.add_scalar('Training Accuracy(Every 10 batch)', acc.item(), epoch * NUM_BATCHES + batch_idx)\n",
    "            \n",
    "            \n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (features, targets) in enumerate(val_loader):\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # forward\n",
    "            logits = model(features)\n",
    "            loss = loss_fn(logits, targets)\n",
    "            acc = accuracy(logits, targets)\n",
    "\n",
    "            # log validation\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{NUM_EPOCHS}], Step [{batch_idx}/{NUM_BATCHES_VAL}], Loss: {loss.item():.4f}', f'Accuracy: {acc.item():.4f}')\n",
    "                writer.add_scalar('Validation Loss (10 batch)', loss.item(), epoch * NUM_BATCHES_VAL + batch_idx) # Loss every 10 batch\n",
    "                writer.add_scalar('Validation Accuracy (10 batch)', acc.item(), epoch * NUM_BATCHES_VAL + batch_idx) # Accuracy every 10 batch\n",
    "\n",
    "    # # Test phase\n",
    "    # model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     for batch_idx, (features, targets) in enumerate(test_loader):\n",
    "    #         features = features.to(device)\n",
    "    #         targets = targets.to(device)\n",
    "    # \n",
    "    #         # forward\n",
    "    #         logits = model(features)\n",
    "    #         loss = loss_fn(logits, targets)\n",
    "    #         acc = accuracy(logits, targets)\n",
    "    # \n",
    "    #         # log test\n",
    "    #         if batch_idx % 100 == 0:\n",
    "    #             print(f'Epoch [{epoch + 1}/{NUM_EPOCHS}], Step [{batch_idx}/{NUM_BATCHES_TEST}], Loss: {loss.item():.4f}, Accuracy: {acc.item():.4f}')\n",
    "    #             writer.add_scalar('test loss', loss.item(), epoch * NUM_BATCHES_TEST + batch_idx)\n",
    "    #             writer.add_scalar('test accuracy', acc.item(), epoch * NUM_BATCHES_TEST + batch_idx)\n",
    "    \n",
    "\n",
    "# clear cache\n",
    "torch.cuda.empty_cache()\n",
    "features = None\n",
    "targets = None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T17:56:07.707372135Z",
     "start_time": "2023-12-31T17:41:31.777192965Z"
    }
   },
   "id": "8375ff1459203cc2",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to AlexNet_v1_2024_01_01-00_41_31.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Save the model checkpoint\n",
    "if not os.path.exists('models'):\n",
    "    os.mkdir('models')\n",
    "VERSION = 1\n",
    "MODEL_NAME = f'AlexNet_v{VERSION}_{date_time}.ckpt'\n",
    "torch.save(model.state_dict(), os.path.join('models', MODEL_NAME))\n",
    "print(f'Saved PyTorch Model State to {MODEL_NAME}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T17:56:07.912098658Z",
     "start_time": "2023-12-31T17:56:07.709008457Z"
    }
   },
   "id": "5d4680b684d91c4e",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 99.23%\n"
     ]
    }
   ],
   "source": [
    "# Test the model load the model checkpoint\n",
    "model_loaded = AlexNet().to(device)\n",
    "\n",
    "# Load the model checkpoint\n",
    "model_loaded.load_state_dict(torch.load(os.path.join('models', MODEL_NAME)))\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "model_loaded.eval()\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for features, targets in test_loader:\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        logits = model_loaded(features)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        writer.add_scalar('test accuracy', 100 * correct / total, 0)\n",
    "\n",
    "    print(f'Accuracy of the model on the test images: {100 * correct / total}%')\n",
    "    \n",
    "features = None\n",
    "targets = None\n",
    "\n",
    "\n",
    "# Close the writer\n",
    "writer.flush()\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T17:56:18.079230346Z",
     "start_time": "2023-12-31T17:56:07.910760339Z"
    }
   },
   "id": "26bf759aa71e8e04",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Released all variables\n"
     ]
    }
   ],
   "source": [
    "model = None\n",
    "model_loaded = None\n",
    "\n",
    "# release all loaders\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "# release all variables\n",
    "optimizer = None\n",
    "loss_fn = None\n",
    "accuracy = None\n",
    "\n",
    "# Clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print('Released all variables')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T17:56:18.090821288Z",
     "start_time": "2023-12-31T17:56:18.077258620Z"
    }
   },
   "id": "1fa78bb6c251d7c2",
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
