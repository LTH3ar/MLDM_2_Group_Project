{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Importing Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a5c64051bd059b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-31 15:04:04.313578: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-31 15:04:04.313615: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-31 15:04:04.314568: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-31 15:04:04.319580: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-31 15:04:05.044572: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torchmetrics import Accuracy\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:04:06.102471414Z",
     "start_time": "2023-12-31T08:04:03.089400171Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Setting Device"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b57f7f91e5c6b06a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Setting Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:04:06.157200486Z",
     "start_time": "2023-12-31T08:04:06.155594185Z"
    }
   },
   "id": "47189a400c9fa3c7",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Preparing Input Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c05cef9cb03b952"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Preparing Input Data\n",
    "# prepare the dataset MNIST(1x28x28) -> (3x224x224) for AlexNet\n",
    "# Upscale the grayscale images to RGB size\n",
    "upscale_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to 3-channel grayscale\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.1307], std=[0.3081])  # Normalize to [-1, 1] range\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:04:06.185385466Z",
     "start_time": "2023-12-31T08:04:06.159161189Z"
    }
   },
   "id": "43eb54b34ab07e86",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_val_dataset length: 60000\n",
      "test_dataset length: 10000\n",
      "train_val_dataset shape: torch.Size([3, 224, 224])\n",
      "test_dataset shape: torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "train_val_dataset = datasets.MNIST(root='./dataset', train=True, transform=upscale_transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./dataset', train=False, transform=upscale_transform, download=True)\n",
    "\n",
    "# Dataset summary\n",
    "print('train_val_dataset length:', len(train_val_dataset))\n",
    "print('test_dataset length:', len(test_dataset))\n",
    "print('train_val_dataset shape:', train_val_dataset[0][0].shape)\n",
    "print('test_dataset shape:', test_dataset[0][0].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:04:06.220837967Z",
     "start_time": "2023-12-31T08:04:06.177439630Z"
    }
   },
   "id": "54b4b930857f8cff",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset length: 48000\n",
      "val_dataset length: 12000\n",
      "test_dataset length: 10000\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train and validation\n",
    "train_size = int(0.8 * len(train_val_dataset))\n",
    "val_size = len(train_val_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_val_dataset, [train_size, val_size])\n",
    "\n",
    "# Dataset summary\n",
    "print('train_dataset length:', len(train_dataset))\n",
    "print('val_dataset length:', len(val_dataset))\n",
    "print('test_dataset length:', len(test_dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:04:06.222542235Z",
     "start_time": "2023-12-31T08:04:06.220568116Z"
    }
   },
   "id": "e37052e65ce41d98",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader length: 375\n",
      "val_loader length: 94\n",
      "test_loader length: 79\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "BATCH_SIZE = 128 if torch.cuda.is_available() else 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# dataloaders summary\n",
    "print('train_loader length:', len(train_loader))\n",
    "print('val_loader length:', len(val_loader))\n",
    "print('test_loader length:', len(test_loader))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:04:06.267423349Z",
     "start_time": "2023-12-31T08:04:06.224387108Z"
    }
   },
   "id": "2408fdd9969a3431",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Defining Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b4a9d85fc9feec9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define the model AlexNet specific for the transformed MNIST\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # ============================================================================== #\n",
    "            # 1st conv layer\n",
    "            # input: 3x224x224 (upscaled from 1x28x28)\n",
    "            # output: 96x55x55\n",
    "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=0, ),\n",
    "            # activation function: ReLU\n",
    "            nn.ReLU(),\n",
    "            # max pooling layer with kernel size 3 and stride 2\n",
    "            # output: 96x27x27\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 2nd conv layer\n",
    "            # input: 96x27x27\n",
    "            # output: 256x27x27\n",
    "            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2),\n",
    "            # activation function: ReLU\n",
    "            nn.ReLU(),\n",
    "            # max pooling layer with kernel size 3 and stride 2\n",
    "            # output: 256x13x13\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 3rd conv layer\n",
    "            # input: 256x13x13\n",
    "            # output: 384x13x13\n",
    "            nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
    "            # activation function: ReLU\n",
    "            nn.ReLU(),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 4th conv layer\n",
    "            # input: 384x13x13\n",
    "            # output: 384x13x13\n",
    "            nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
    "            # activation function: ReLU\n",
    "            nn.ReLU(),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 5th conv layer\n",
    "            # input: 384x13x13\n",
    "            # output: 256x13x13\n",
    "            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            # activation function: ReLU\n",
    "            nn.ReLU(),\n",
    "            # max pooling layer with kernel size 3 and stride 2\n",
    "            # output: 256x6x6\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "            # ============================================================================== #\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            # flatten\n",
    "            nn.Flatten(), # 256*5*5 = 6400\n",
    "            # ============================================================================== #\n",
    "            # 1st fc layer Dense: 4096 fully connected neurons\n",
    "            nn.Dropout(p=0.5), # dropout layer with p=0.5\n",
    "            nn.Linear(in_features=256 * 5 * 5, out_features=4096), # 256*5*5\n",
    "            nn.ReLU(),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 2nd fc layer Dense: 4096 fully connected neurons\n",
    "            nn.Dropout(p=0.5), # dropout layer with p=0.5\n",
    "            nn.Linear(in_features=4096, out_features=4096), # 4096\n",
    "            nn.ReLU(),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 3rd fc layer Dense: 10 fully connected neurons\n",
    "            nn.Linear(in_features=4096, out_features=num_classes) # 4096\n",
    "            # ============================================================================== #\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:04:06.268144575Z",
     "start_time": "2023-12-31T08:04:06.267256824Z"
    }
   },
   "id": "459d3e93ddc78a53",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU()\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Dropout(p=0.5, inplace=False)\n",
      "    (2): Linear(in_features=6400, out_features=4096, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "    (5): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "model = AlexNet().to(device)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:04:06.620852055Z",
     "start_time": "2023-12-31T08:04:06.267425874Z"
    }
   },
   "id": "98615f7aa89f388c",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "AlexNet                                  [1, 10]                   --\n",
      "├─Sequential: 1-1                        [1, 256, 5, 5]            --\n",
      "│    └─0.weight                                                    ├─34,848\n",
      "│    └─0.bias                                                      ├─96\n",
      "│    └─3.weight                                                    ├─614,400\n",
      "│    └─3.bias                                                      ├─256\n",
      "│    └─6.weight                                                    ├─884,736\n",
      "│    └─6.bias                                                      ├─384\n",
      "│    └─8.weight                                                    ├─1,327,104\n",
      "│    └─8.bias                                                      ├─384\n",
      "│    └─10.weight                                                   ├─884,736\n",
      "│    └─10.bias                                                     └─256\n",
      "│    └─Conv2d: 2-1                       [1, 96, 54, 54]           34,944\n",
      "│    │    └─weight                                                 ├─34,848\n",
      "│    │    └─bias                                                   └─96\n",
      "│    └─ReLU: 2-2                         [1, 96, 54, 54]           --\n",
      "│    └─MaxPool2d: 2-3                    [1, 96, 26, 26]           --\n",
      "│    └─Conv2d: 2-4                       [1, 256, 26, 26]          614,656\n",
      "│    │    └─weight                                                 ├─614,400\n",
      "│    │    └─bias                                                   └─256\n",
      "│    └─ReLU: 2-5                         [1, 256, 26, 26]          --\n",
      "│    └─MaxPool2d: 2-6                    [1, 256, 12, 12]          --\n",
      "│    └─Conv2d: 2-7                       [1, 384, 12, 12]          885,120\n",
      "│    │    └─weight                                                 ├─884,736\n",
      "│    │    └─bias                                                   └─384\n",
      "│    └─ReLU: 2-8                         [1, 384, 12, 12]          --\n",
      "│    └─Conv2d: 2-9                       [1, 384, 12, 12]          1,327,488\n",
      "│    │    └─weight                                                 ├─1,327,104\n",
      "│    │    └─bias                                                   └─384\n",
      "│    └─ReLU: 2-10                        [1, 384, 12, 12]          --\n",
      "│    └─Conv2d: 2-11                      [1, 256, 12, 12]          884,992\n",
      "│    │    └─weight                                                 ├─884,736\n",
      "│    │    └─bias                                                   └─256\n",
      "│    └─ReLU: 2-12                        [1, 256, 12, 12]          --\n",
      "│    └─MaxPool2d: 2-13                   [1, 256, 5, 5]            --\n",
      "├─Sequential: 1-2                        [1, 10]                   --\n",
      "│    └─2.weight                                                    ├─26,214,400\n",
      "│    └─2.bias                                                      ├─4,096\n",
      "│    └─5.weight                                                    ├─16,777,216\n",
      "│    └─5.bias                                                      ├─4,096\n",
      "│    └─7.weight                                                    ├─40,960\n",
      "│    └─7.bias                                                      └─10\n",
      "│    └─Flatten: 2-14                     [1, 6400]                 --\n",
      "│    └─Dropout: 2-15                     [1, 6400]                 --\n",
      "│    └─Linear: 2-16                      [1, 4096]                 26,218,496\n",
      "│    │    └─weight                                                 ├─26,214,400\n",
      "│    │    └─bias                                                   └─4,096\n",
      "│    └─ReLU: 2-17                        [1, 4096]                 --\n",
      "│    └─Dropout: 2-18                     [1, 4096]                 --\n",
      "│    └─Linear: 2-19                      [1, 4096]                 16,781,312\n",
      "│    │    └─weight                                                 ├─16,777,216\n",
      "│    │    └─bias                                                   └─4,096\n",
      "│    └─ReLU: 2-20                        [1, 4096]                 --\n",
      "│    └─Linear: 2-21                      [1, 10]                   40,970\n",
      "│    │    └─weight                                                 ├─40,960\n",
      "│    │    └─bias                                                   └─10\n",
      "==========================================================================================\n",
      "Total params: 46,787,978\n",
      "Trainable params: 46,787,978\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 1.01\n",
      "==========================================================================================\n",
      "Input size (MB): 0.60\n",
      "Forward/backward pass size (MB): 4.87\n",
      "Params size (MB): 187.15\n",
      "Estimated Total Size (MB): 192.62\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nAlexNet                                  [1, 10]                   --\n├─Sequential: 1-1                        [1, 256, 5, 5]            --\n│    └─0.weight                                                    ├─34,848\n│    └─0.bias                                                      ├─96\n│    └─3.weight                                                    ├─614,400\n│    └─3.bias                                                      ├─256\n│    └─6.weight                                                    ├─884,736\n│    └─6.bias                                                      ├─384\n│    └─8.weight                                                    ├─1,327,104\n│    └─8.bias                                                      ├─384\n│    └─10.weight                                                   ├─884,736\n│    └─10.bias                                                     └─256\n│    └─Conv2d: 2-1                       [1, 96, 54, 54]           34,944\n│    │    └─weight                                                 ├─34,848\n│    │    └─bias                                                   └─96\n│    └─ReLU: 2-2                         [1, 96, 54, 54]           --\n│    └─MaxPool2d: 2-3                    [1, 96, 26, 26]           --\n│    └─Conv2d: 2-4                       [1, 256, 26, 26]          614,656\n│    │    └─weight                                                 ├─614,400\n│    │    └─bias                                                   └─256\n│    └─ReLU: 2-5                         [1, 256, 26, 26]          --\n│    └─MaxPool2d: 2-6                    [1, 256, 12, 12]          --\n│    └─Conv2d: 2-7                       [1, 384, 12, 12]          885,120\n│    │    └─weight                                                 ├─884,736\n│    │    └─bias                                                   └─384\n│    └─ReLU: 2-8                         [1, 384, 12, 12]          --\n│    └─Conv2d: 2-9                       [1, 384, 12, 12]          1,327,488\n│    │    └─weight                                                 ├─1,327,104\n│    │    └─bias                                                   └─384\n│    └─ReLU: 2-10                        [1, 384, 12, 12]          --\n│    └─Conv2d: 2-11                      [1, 256, 12, 12]          884,992\n│    │    └─weight                                                 ├─884,736\n│    │    └─bias                                                   └─256\n│    └─ReLU: 2-12                        [1, 256, 12, 12]          --\n│    └─MaxPool2d: 2-13                   [1, 256, 5, 5]            --\n├─Sequential: 1-2                        [1, 10]                   --\n│    └─2.weight                                                    ├─26,214,400\n│    └─2.bias                                                      ├─4,096\n│    └─5.weight                                                    ├─16,777,216\n│    └─5.bias                                                      ├─4,096\n│    └─7.weight                                                    ├─40,960\n│    └─7.bias                                                      └─10\n│    └─Flatten: 2-14                     [1, 6400]                 --\n│    └─Dropout: 2-15                     [1, 6400]                 --\n│    └─Linear: 2-16                      [1, 4096]                 26,218,496\n│    │    └─weight                                                 ├─26,214,400\n│    │    └─bias                                                   └─4,096\n│    └─ReLU: 2-17                        [1, 4096]                 --\n│    └─Dropout: 2-18                     [1, 4096]                 --\n│    └─Linear: 2-19                      [1, 4096]                 16,781,312\n│    │    └─weight                                                 ├─16,777,216\n│    │    └─bias                                                   └─4,096\n│    └─ReLU: 2-20                        [1, 4096]                 --\n│    └─Linear: 2-21                      [1, 10]                   40,970\n│    │    └─weight                                                 ├─40,960\n│    │    └─bias                                                   └─10\n==========================================================================================\nTotal params: 46,787,978\nTrainable params: 46,787,978\nNon-trainable params: 0\nTotal mult-adds (Units.GIGABYTES): 1.01\n==========================================================================================\nInput size (MB): 0.60\nForward/backward pass size (MB): 4.87\nParams size (MB): 187.15\nEstimated Total Size (MB): 192.62\n=========================================================================================="
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model summary\n",
    "# Detailed layer-wise summary\n",
    "summary(model, input_size=(1, 3, 224, 224), verbose=2, device=device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:04:06.733459383Z",
     "start_time": "2023-12-31T08:04:06.622644599Z"
    }
   },
   "id": "c628ab4d26ae84d5",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Optimizer and loss function\n",
    "LEARNING_RATE = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "accuracy = Accuracy(task='multiclass', num_classes=10).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:04:06.777295389Z",
     "start_time": "2023-12-31T08:04:06.735273138Z"
    }
   },
   "id": "f9d48a09cc8fecc4",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e68e261f5fc5bec"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Training\n",
    "# Log training process to TensorBoard\n",
    "date_time = datetime.datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "log_dir = os.path.join('logs', date_time)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:04:06.777791759Z",
     "start_time": "2023-12-31T08:04:06.777234764Z"
    }
   },
   "id": "f0e84690ff381c92",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 128 if torch.cuda.is_available() else 64\n",
    "NUM_BATCHES = len(train_loader)\n",
    "NUM_BATCHES_VAL = len(val_loader)\n",
    "NUM_BATCHES_TEST = len(test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:04:06.778315661Z",
     "start_time": "2023-12-31T08:04:06.777360633Z"
    }
   },
   "id": "160833a284c81c6f",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [0/375], Loss: 2.3022\n",
      "Epoch [1/10], Step [100/375], Loss: 0.1913\n",
      "Epoch [1/10], Step [200/375], Loss: 0.0545\n",
      "Epoch [1/10], Step [300/375], Loss: 0.1809\n",
      "Epoch [1/10], Step [0/94], Loss: 0.1248 Accuracy: 0.9609\n",
      "Epoch [2/10], Step [0/375], Loss: 0.0829\n",
      "Epoch [2/10], Step [100/375], Loss: 0.0411\n",
      "Epoch [2/10], Step [200/375], Loss: 0.0393\n",
      "Epoch [2/10], Step [300/375], Loss: 0.0518\n",
      "Epoch [2/10], Step [0/94], Loss: 0.1465 Accuracy: 0.9609\n",
      "Epoch [3/10], Step [0/375], Loss: 0.0216\n",
      "Epoch [3/10], Step [100/375], Loss: 0.0697\n",
      "Epoch [3/10], Step [200/375], Loss: 0.0251\n",
      "Epoch [3/10], Step [300/375], Loss: 0.0980\n",
      "Epoch [3/10], Step [0/94], Loss: 0.1380 Accuracy: 0.9688\n",
      "Epoch [4/10], Step [0/375], Loss: 0.0699\n",
      "Epoch [4/10], Step [100/375], Loss: 0.1534\n",
      "Epoch [4/10], Step [200/375], Loss: 0.0526\n",
      "Epoch [4/10], Step [300/375], Loss: 0.0374\n",
      "Epoch [4/10], Step [0/94], Loss: 0.0323 Accuracy: 0.9922\n",
      "Epoch [5/10], Step [0/375], Loss: 0.0477\n",
      "Epoch [5/10], Step [100/375], Loss: 0.0847\n",
      "Epoch [5/10], Step [200/375], Loss: 0.0569\n",
      "Epoch [5/10], Step [300/375], Loss: 0.0935\n",
      "Epoch [5/10], Step [0/94], Loss: 0.1607 Accuracy: 0.9766\n",
      "Epoch [6/10], Step [0/375], Loss: 0.1279\n",
      "Epoch [6/10], Step [100/375], Loss: 0.0604\n",
      "Epoch [6/10], Step [200/375], Loss: 0.0111\n",
      "Epoch [6/10], Step [300/375], Loss: 0.1162\n",
      "Epoch [6/10], Step [0/94], Loss: 0.0503 Accuracy: 0.9844\n",
      "Epoch [7/10], Step [0/375], Loss: 0.0221\n",
      "Epoch [7/10], Step [100/375], Loss: 0.0544\n",
      "Epoch [7/10], Step [200/375], Loss: 0.0837\n",
      "Epoch [7/10], Step [300/375], Loss: 0.0164\n",
      "Epoch [7/10], Step [0/94], Loss: 0.1304 Accuracy: 0.9688\n",
      "Epoch [8/10], Step [0/375], Loss: 0.0330\n",
      "Epoch [8/10], Step [100/375], Loss: 0.0155\n",
      "Epoch [8/10], Step [200/375], Loss: 0.0843\n",
      "Epoch [8/10], Step [300/375], Loss: 0.0439\n",
      "Epoch [8/10], Step [0/94], Loss: 0.0896 Accuracy: 0.9844\n",
      "Epoch [9/10], Step [0/375], Loss: 0.0622\n",
      "Epoch [9/10], Step [100/375], Loss: 0.2424\n",
      "Epoch [9/10], Step [200/375], Loss: 0.0045\n",
      "Epoch [9/10], Step [300/375], Loss: 0.1243\n",
      "Epoch [9/10], Step [0/94], Loss: 0.0665 Accuracy: 0.9766\n",
      "Epoch [10/10], Step [0/375], Loss: 0.0134\n",
      "Epoch [10/10], Step [100/375], Loss: 0.0554\n",
      "Epoch [10/10], Step [200/375], Loss: 0.0254\n",
      "Epoch [10/10], Step [300/375], Loss: 0.0156\n",
      "Epoch [10/10], Step [0/94], Loss: 0.0766 Accuracy: 0.9688\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # forward\n",
    "        logits = model(features)\n",
    "        loss = loss_fn(logits, targets)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "        # log training\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{NUM_EPOCHS}], Step [{batch_idx}/{NUM_BATCHES}], Loss: {loss.item():.4f}')\n",
    "            writer.add_scalar('training loss', loss.item(), epoch * NUM_BATCHES + batch_idx)\n",
    "            \n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (features, targets) in enumerate(val_loader):\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # forward\n",
    "            logits = model(features)\n",
    "            loss = loss_fn(logits, targets)\n",
    "            acc = accuracy(logits, targets)\n",
    "\n",
    "            # log validation\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{NUM_EPOCHS}], Step [{batch_idx}/{NUM_BATCHES_VAL}], Loss: {loss.item():.4f}', f'Accuracy: {acc.item():.4f}')\n",
    "                writer.add_scalar('validation loss', loss.item(), epoch * NUM_BATCHES_VAL + batch_idx)\n",
    "                writer.add_scalar('validation accuracy', acc.item(), epoch * NUM_BATCHES_VAL + batch_idx)\n",
    "\n",
    "    # # Test phase\n",
    "    # model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     for batch_idx, (features, targets) in enumerate(test_loader):\n",
    "    #         features = features.to(device)\n",
    "    #         targets = targets.to(device)\n",
    "    # \n",
    "    #         # forward\n",
    "    #         logits = model(features)\n",
    "    #         loss = loss_fn(logits, targets)\n",
    "    #         acc = accuracy(logits, targets)\n",
    "    # \n",
    "    #         # log test\n",
    "    #         if batch_idx % 100 == 0:\n",
    "    #             print(f'Epoch [{epoch + 1}/{NUM_EPOCHS}], Step [{batch_idx}/{NUM_BATCHES_TEST}], Loss: {loss.item():.4f}, Accuracy: {acc.item():.4f}')\n",
    "    #             writer.add_scalar('test loss', loss.item(), epoch * NUM_BATCHES_TEST + batch_idx)\n",
    "    #             writer.add_scalar('test accuracy', acc.item(), epoch * NUM_BATCHES_TEST + batch_idx)\n",
    "    \n",
    "\n",
    "# clear cache\n",
    "torch.cuda.empty_cache()\n",
    "features = None\n",
    "targets = None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:15:47.098617891Z",
     "start_time": "2023-12-31T08:04:06.777459770Z"
    }
   },
   "id": "8375ff1459203cc2",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Save the model checkpoint\n",
    "if not os.path.exists('models'):\n",
    "    os.mkdir('models')\n",
    "VERSION = 1\n",
    "torch.save(model.state_dict(), os.path.join('models', f'model_v{VERSION}_{date_time}.ckpt'))\n",
    "print('Saved PyTorch Model State to model.ckpt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:15:47.290665672Z",
     "start_time": "2023-12-31T08:15:47.100145949Z"
    }
   },
   "id": "5d4680b684d91c4e",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 98.92%\n"
     ]
    }
   ],
   "source": [
    "# Test the model load the model checkpoint\n",
    "model_loaded = AlexNet().to(device)\n",
    "\n",
    "# Load the model checkpoint\n",
    "model_loaded.load_state_dict(torch.load(os.path.join('models', f'model_v{VERSION}_{date_time}.ckpt')))\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "model_loaded.eval()\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for features, targets in test_loader:\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        logits = model_loaded(features)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        writer.add_scalar('test accuracy', 100 * correct / total, 0)\n",
    "\n",
    "    print(f'Accuracy of the model on the test images: {100 * correct / total}%')\n",
    "    \n",
    "features = None\n",
    "targets = None\n",
    "\n",
    "\n",
    "# Close the writer\n",
    "writer.flush()\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:15:57.223259824Z",
     "start_time": "2023-12-31T08:15:47.293176217Z"
    }
   },
   "id": "26bf759aa71e8e04",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = None\n",
    "model_loaded = None\n",
    "\n",
    "# release all loaders\n",
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "# release all variables\n",
    "optimizer = None\n",
    "loss_fn = None\n",
    "accuracy = None\n",
    "\n",
    "# Clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print('Released all variables')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:15:57.230851492Z",
     "start_time": "2023-12-31T08:15:57.222522691Z"
    }
   },
   "id": "1fa78bb6c251d7c2",
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
