{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Importing Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "691262bc10e94dab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torchmetrics import Accuracy\n",
    "from torchinfo import summary\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6e6f67c794852c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Setting up the device"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90af65f988fe5ae0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setting Device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a8f82fecb194e7e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Preparing the test_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8335ee7ed0eb8ce0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Preparing Input Data\n",
    "# prepare the dataset MNIST(1x28x28) -> (3x224x224) for AlexNet\n",
    "# Upscale the grayscale images to RGB size\n",
    "param_transform = transforms.Compose([\n",
    "    transforms.Pad(2),\n",
    "    transforms.Resize((227, 227)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to 3-channel grayscale\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.1307], std=[0.3081])  # Normalize to [-1, 1] range\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1167ff33f16434f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "test_data = datasets.MNIST(\n",
    "    root='./dataset', \n",
    "    train=False, \n",
    "    transform=param_transform, \n",
    "    download=True)\n",
    "\n",
    "test_data_info = datasets.MNIST(\n",
    "    root='./dataset', \n",
    "    train=True,\n",
    "    download=False)\n",
    "print(\"MNIST dataset information:\")\n",
    "print(\" - Number of training samples: {}\".format(len(test_data_info)))\n",
    "print(\" - Number of test samples: {}\".format(len(test_data)))\n",
    "print(\" - Image Shape: {}\".format(test_data[0][0].shape))\n",
    "print(\" - Number of classes: {}\".format(len(test_data.classes)))\n",
    "print(\" - Samples number all classes: {}\".format(test_data.targets.bincount()))\n",
    "# info about the dataset\n",
    "print(test_data_info)\n",
    "\n",
    "# plot the train dataset samples count for each class\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.bar(test_data_info.classes, test_data_info.targets.bincount())\n",
    "plt.title('Train dataset samples count for each class')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Samples count')\n",
    "plt.savefig('train_dataset_samples_count.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Dataset summary\n",
    "print('Test dataset:')\n",
    "print(' - Number of datapoints: {}'.format(len(test_data)))\n",
    "print(' - Image Shape: {}'.format(test_data[0][0].shape))\n",
    "print(' - Number of classes: {}'.format(len(test_data.classes)))\n",
    "print(\" - Samples number all classes: {}\".format(test_data.targets.bincount()))\n",
    "\n",
    "# plot the test dataset samples count for each class\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.bar(test_data.classes, test_data.targets.bincount())\n",
    "plt.title('Test dataset samples count for each class')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Samples count')\n",
    "plt.savefig('test_dataset_samples_count.png')\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46a9b95359ff9136"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dataset for visualization\n",
    "# get the 10 images from the test set for visualization\n",
    "vis_size = 10\n",
    "test_data_vis = torch.utils.data.Subset(test_data, range(vis_size))\n",
    "\n",
    "# Dataset summary\n",
    "print('Test dataset for visualization:')\n",
    "print(' - Number of datapoints: {}'.format(len(test_data_vis)))\n",
    "print(' - Image Shape: {}'.format(test_data_vis[0][0].shape))\n",
    "\n",
    "# loader\n",
    "test_loader_vis = DataLoader(test_data_vis, batch_size=vis_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "569e566a50d77a27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "if torch.cuda.is_available():\n",
    "    BATCH_SIZE = 128\n",
    "elif torch.backends.mps.is_available():\n",
    "    BATCH_SIZE = 128\n",
    "else:\n",
    "    BATCH_SIZE = 64\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Dataloader summary\n",
    "print('Test dataloader:')\n",
    "print(' - Number of batches: {}'.format(len(test_loader)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbcb849c1ad403be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Loading the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0be4bb6fd1e5b4b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the model AlexNet specific for the transformed MNIST\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # ============================================================================== #\n",
    "            # 1st conv layer\n",
    "            # input: 3x224x224 (upscaled from 1x28x28)\n",
    "            # output: 96x55x55\n",
    "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=0, ),\n",
    "            # activation function: ReLU\n",
    "            nn.ReLU(),\n",
    "            # max pooling layer with kernel size 3 and stride 2\n",
    "            # output: 96x27x27\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 2nd conv layer\n",
    "            # input: 96x27x27\n",
    "            # output: 256x27x27\n",
    "            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2),\n",
    "            # activation function: ReLU\n",
    "            nn.ReLU(),\n",
    "            # max pooling layer with kernel size 3 and stride 2\n",
    "            # output: 256x13x13\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 3rd conv layer\n",
    "            # input: 256x13x13\n",
    "            # output: 384x13x13\n",
    "            nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
    "            # activation function: ReLU\n",
    "            nn.ReLU(),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 4th conv layer\n",
    "            # input: 384x13x13\n",
    "            # output: 384x13x13\n",
    "            nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1),\n",
    "            # activation function: ReLU\n",
    "            nn.ReLU(),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 5th conv layer\n",
    "            # input: 384x13x13\n",
    "            # output: 256x13x13\n",
    "            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            # activation function: ReLU\n",
    "            nn.ReLU(),\n",
    "            # max pooling layer with kernel size 3 and stride 2\n",
    "            # output: 256x6x6\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "            # ============================================================================== #\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            # flatten\n",
    "            nn.Flatten(), # 256*5*5 = 6400\n",
    "            # ============================================================================== #\n",
    "            # 1st fc layer Dense: 4096 fully connected neurons\n",
    "            nn.Dropout(p=0.5), # dropout layer with p=0.5\n",
    "            nn.Linear(in_features=256 * 6 * 6, out_features=4096), # 256*5*5\n",
    "            nn.ReLU(),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 2nd fc layer Dense: 4096 fully connected neurons\n",
    "            nn.Dropout(p=0.5), # dropout layer with p=0.5\n",
    "            nn.Linear(in_features=4096, out_features=4096), # 4096\n",
    "            nn.ReLU(),\n",
    "            # ============================================================================== #\n",
    "            \n",
    "            # ============================================================================== #\n",
    "            # 3rd fc layer Dense: 10 fully connected neurons\n",
    "            nn.Linear(in_features=4096, out_features=num_classes) # 4096\n",
    "            # ============================================================================== #\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a146bdd6d954f261"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "accuracy = Accuracy(task='multiclass', num_classes=10).to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a46f17406aded3f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Log the test\n",
    "date_time = datetime.datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2667b90077ce22c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models_lst = os.listdir('models')\n",
    "models_lst.sort()\n",
    "print(models_lst)\n",
    "# create the log directory\n",
    "test_folder_name = f\"test_{date_time}\"\n",
    "log_dir = os.path.join('test_logs', test_folder_name)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ebe163c644e1c65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test the model\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "loss_lst = []\n",
    "acc_lst = []\n",
    "for model_name in models_lst:\n",
    "    # Load the model\n",
    "    model = AlexNet().to(device)\n",
    "    # Load the model weights\n",
    "    model.load_state_dict(torch.load(os.path.join('models', model_name), map_location=device))\n",
    "    # Model summary\n",
    "    # summary(model, input_size=(1, 1, 28, 28), verbose=2, device=device)\n",
    "    summary(model, input_size=(1, 3, 227, 227), verbose=2, device=device)\n",
    "    # Calculate model weights size\n",
    "    model_size = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'Model size: {model_size} parameters')\n",
    "    print('Model weights size: {:.2f} MB'.format(sum(p.numel() for p in model.parameters()) / (1024 * 1024)))\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss_tmp = []\n",
    "        y_true_tmp = []\n",
    "        y_pred_tmp = []\n",
    "        y_true_cfm = []\n",
    "        y_pred_cfm = []\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            # Move the data to device\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "\n",
    "            # save the true and predicted labels for confusion matrix\n",
    "            y_true_tmp.append(target)\n",
    "            y_pred_tmp.append(output)\n",
    "\n",
    "            # save the true and predicted labels for confusion matrix\n",
    "            y_true_cfm.extend(target.cpu().numpy())\n",
    "            y_pred_cfm.extend(torch.argmax(output, dim=1).cpu().numpy())\n",
    "\n",
    "        # save the true and predicted labels for confusion matrix, convert to numpy array\n",
    "        y_true.append(y_true_cfm)\n",
    "        y_pred.append(y_pred_cfm)\n",
    "\n",
    "        # Calculate the average accuracy\n",
    "        avg_acc = accuracy(torch.cat(y_pred_tmp), torch.cat(y_true_tmp)).item()*100\n",
    "        avg_loss = loss_fn(torch.cat(y_pred_tmp), torch.cat(y_true_tmp)).item()\n",
    "    print('Test: Average accuracy: {:.2f}%'.format(avg_acc))\n",
    "    print('Test: Average loss: {:.2f}'.format(avg_loss))\n",
    "    # for each model add the accuracy and loss to the tensorboard so that we can compare them like a line\n",
    "    acc_lst.append(avg_acc)\n",
    "    loss_lst.append(avg_loss)\n",
    "\n",
    "    # clean the GPU memory\n",
    "    # clear cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    elif torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    else:\n",
    "        torch.empty_cache()\n",
    "    # release the model from GPU memory\n",
    "    model = None\n",
    "\n",
    "for i in range(len(models_lst)):\n",
    "    writer.add_scalar(tag=\"Accuracy\", scalar_value=acc_lst[i], global_step=int((i + 1) * 10))\n",
    "    writer.add_scalar(tag=\"Loss\", scalar_value=loss_lst[i], global_step=int((i + 1) * 10))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "586ac65d54627775"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Confusion matrix all test data\n",
    "for i in range(len(models_lst)):\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true[i], y_pred[i])\n",
    "    print(cm)\n",
    "    # Classification report\n",
    "    print(classification_report(y_true[i], y_pred[i], target_names=test_data.classes))\n",
    "    # Plot the confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.savefig(os.path.join(log_dir, f'confusion_matrix_{models_lst[i]}.png'))\n",
    "    plt.show()\n",
    "    with open(os.path.join(log_dir, f'classification_report_{models_lst[i]}.txt'), 'w') as f:\n",
    "        f.write(classification_report(y_true[i], y_pred[i], target_names=test_data.classes))\n",
    "        f.write('\\n')\n",
    "        f.write('Confusion matrix:\\n')\n",
    "        f.write(str(cm))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a7c60a64f052f42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize the bar chart accuracy of models, V1, V2, V3, V4 (range 97-100%) add acc value to the top of the bar\n",
    "labels = []\n",
    "for i in range(len(models_lst)):\n",
    "    labels.append(f'V{i + 1}')\n",
    "print(labels)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(labels, acc_lst)\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_title('Accuracy of models')\n",
    "ax.set_ylim([97, 100])\n",
    "for i, v in enumerate(acc_lst):\n",
    "    ax.text(i - 0.1, v + 0.1, str(round(v, 2)), color='black', fontweight='bold')\n",
    "plt.savefig(os.path.join(log_dir, 'accuracy_of_models.png'))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48a1f4a7ffb92db0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Close the writer\n",
    "writer.flush()\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "211a9ee7dbc40da7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Release GPU memory cache\n",
    "# clear cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "elif torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "else:\n",
    "    torch.empty_cache()\n",
    "\n",
    "# Release model from GPU memory\n",
    "model = None\n",
    "\n",
    "# release all loaded data\n",
    "data_vis = None\n",
    "target_vis = None\n",
    "data = None\n",
    "target = None\n",
    "data_loader = None\n",
    "data_loader_vis = None\n",
    "test_data = None\n",
    "test_data_vis = None\n",
    "test_loader = None\n",
    "test_loader_vis = None\n",
    "\n",
    "print('Done!')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7d3545a13ac8127"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
